{
  "arxiv_id": "2508.15859",
  "title": "Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language",
  "authors": [
    "Taniguchi, Tadahiro"
  ],
  "date": "2025/08/20",
  "abstract": "This commentary extends the discussion by Parr et al. on memory and attention beyond individual cognitive systems. From the perspective of the Collective Predictive Coding (CPC) hypothesis -- a framework for understanding these faculties and the emergence of language at the group level -- we introduce a hypothetical idea: that language, with its embedded distributional semantics, serves as a collectively formed external representation. CPC generalises the concepts of individual memory and attention to the collective level. This offers a new perspective on how shared linguistic structures, which may embrace collective world models learned through next-word prediction, emerge from and shape group-level cognition.",
  "origin": "https://arxiv.org/abs/",
  "key": "2508.15859",
  "body": "# Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language\n\n Tadahiro Taniguchi\n Graduate School of Informatics \n Kyoto University\n taniguchi@i.kyoto-u.ac.jp\n\n###### Abstract\n\nThis commentary extends the discussion by Parr et al. on memory and attention beyond individual cognitive systems. From the perspective of the Collective Predictive Coding (CPC) hypothesis‚Äîa framework for understanding these faculties and the emergence of language at the group level‚Äîwe introduce a hypothetical idea: that language, with its embedded distributional semantics, serves as a collectively formed external representation. CPC generalises the concepts of individual memory and attention to the collective level. This offers a new perspective on how shared linguistic structures, which may embrace collective world models learned through next-word prediction, emerge from and shape group-level cognition.\n\n 1 Introduction \n\nLanguage is formed collectively. Each individual cognitive system participating in its emergence possesses memory and attention. These faculties are undoubtedly crucial not only for modeling and understanding language but also for its formation, or emergence.\n\nLanguage possesses a semantic-syntactic structure, and modeling its sequential nature certainly yields significant information. As Parr et¬†al., ( 2025 ) intuitively describe in their contribution to this special issue, mechanisms for storing such information (memory) and for retrieving information therefrom (attention) play vital roles in extracting information from long contexts and sequences with internal structures beyond Markov. Attention mechanisms in neural networks gained prominence through Transformers (Vaswani et¬†al.,, 2017 ) and have been applied to many artificial cognitive systems. Meanwhile, for long-sequence memory, State Space Models like Mamba (Gu and Dao,, 2023 ) have garnered considerable attention.\n\n 2 Main Discussion \n\nThe importance of embedding spaces, as highlighted by Parr et¬†al., ( 2025 ) , is also well-taken. This concept is rooted in linguistic structures that give rise to distributional semantics (Harris,, 1954 ) .\nHowever, a crucial question arises: who embedded the ‚Äúretrievable‚Äù information and structures into the sequential data of language? While embedding spaces are important, who created the linguistic structures that manifest as distributional semantics within these spaces? It is not only individual humans but humans as a collective. This question leads to the broader issue of language emergence, which is intrinsically linked with individual cognitive capabilities concerning language, including memory and attention.\n\nGenerative models, including those based on the free energy principle and active inference, have the potential to extend the discussion beyond the scope of the target paper by Parr et¬†al., ( 2025 ) to these collective questions. The Collective Predictive Coding (CPC) hypothesis (Taniguchi,, 2024 ) describes a hierarchical structure where individuals perform internal representation learning (i.e., world modeling; see Friston et¬†al., ( 2021 ); Taniguchi et¬†al., 2023a ), and simultaneously, the collective performs external representation learning (i.e., language/symbol emergence; see Taniguchi et¬†al., ( 2018 ); Taniguchi, ( 2024 ) ). The entire process can be characterized as (decentralized) Bayesian inference. In particular, the latter inference (external representation learning) is approximated as Bayesian inference through forms of language games, such as the Metropolis-Hastings Naming Game (e.g., Taniguchi et¬†al., 2023b ; Hoang et¬†al., ( 2024 ) ).\n\nFrom this perspective, an agent collective, coupled by the emergent language, can be viewed as a single entity performing active inference or predictive coding. This idea has been extended to model scientific inquiry as Collective Predictive Coding as a Model of Science ( Taniguchi et¬†al., 2025b, ) , deriving the free energy of this human collective and demonstrating its theoretical consistency with the free energy minimization of individual agents introducing a collective regularization term representing semiotic plasticity . Furthermore, it has been hypothesized that language emerging in this manner could exert a top-down influence on individual cognition and consciousness, such as on qualia structure ( Taniguchi et¬†al., 2024a, ) .\n\nThe CPC perspective also offers an answer to the question posed by Parr et¬†al., ( 2025 ) (Section 5.2): ‚ÄúIs next-word prediction the most effective way to learn language?‚Äù. We argue in the affirmative. Next-token prediction is fundamentally about modeling the probability distribution of observations, p ‚Äã ( ùê® ) p(\\mathbf{o}) . If we adopt the CPC viewpoint that language integrates the observations of a group of partially observable agents and structurally represents p ‚Äã ( ùê® ) p(\\mathbf{o}) , then modeling this distribution is the essence of human language learning and, through it, the learning of a collective world model ( Taniguchi et¬†al., 2024b, ) .\n\nNow, let us consider the framework wherein the collective is viewed as a single super-cognitive system, termed System 3 , which extends the well-known System 1/2 framework (Kahneman,, 2011 ) , as proposed in Taniguchi et¬†al., 2025a . From this standpoint, perspectives on language, attention, and memory emerge that transcend the individual. For instance, through written language, society can store information externally, forming a collective memory. Individuals can also access memories stored by others through communication. Regarding attention, this super-cognitive system (the subject of group-level active inference) may be factorized into individual beings. Within their brains, the factorization in attentional mechanisms, as described by Parr et¬†al., ( 2025 ) , might then occur.\n\n 3 Conclusion \n\nThis commentary suggests that the discussion of memory and attention through generative models, mediated by active inference and collective predictive coding, opens avenues for inquiries extending beyond individual-level cognitive science. Such inquiries can shed more light on the mystery of language, which originates from group-level phenomena."
}