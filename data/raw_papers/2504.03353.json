{
  "arxiv_id": "2504.03353",
  "title": "Decentralized Collective World Model for Emergent Communication and Coordination",
  "authors": [
    "Nomura, Kentaro",
    "Aoki, Tatsuya",
    "Taniguchi, Tadahiro",
    "Horii, Takato"
  ],
  "date": "2025/04/04",
  "abstract": "We propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents&#39; internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.",
  "origin": "https://arxiv.org/abs/",
  "key": "2504.03353",
  "body": "Decentralized Collective World Model \n for Emergent Communication and Coordination\n\n Kentaro Nomura 1 , Tatsuya Aoki 1 , Tadahiro Taniguchi 2,3 and Takato Horii 1,4 \n *This work was supported by Japan Science and Technology Agency (JST) Moonshot R&amp;D Grant Number JPMJMS2011. 1 Dept. of Systems Innovation, Graduate School of Engineering Science, The University of Osaka, Osaka, Japan {k.nomura@rlg., t.aoki@rlg., takato@} sys.es.osaka-u.ac.jp 2 Dept. of Informatics, Kyoto University, Kyoto, Japan taniguchi@i.kyoto-u.ac.jp 3 Dept. of Science and Engineering, Ritsumeikan University, Shiga, Japan 4 IRCN, The University of Tokyo, Tokyo, Japan \n\n###### Abstract\n\nWe propose a fully decentralized multi-agent world model that enables both symbol emergence for communication and coordinated behavior through temporal extension of collective predictive coding. Unlike previous research that focuses on either communication or coordination separately, our approach achieves both simultaneously. Our method integrates world models with communication channels, enabling agents to predict environmental dynamics, estimate states from partial observations, and share critical information through bidirectional message exchange with contrastive learning for message alignment. Using a two-agent trajectory drawing task, we demonstrate that our communication-based approach outperforms non-communicative models when agents have divergent perceptual capabilities, achieving the second-best coordination after centralized models. Importantly, our decentralized approach with constraints preventing direct access to other agents‚Äô internal states facilitates the emergence of more meaningful symbol systems that accurately reflect environmental states. These findings demonstrate the effectiveness of decentralized communication for supporting coordination while developing shared representations of the environment.\n\n I Introduction \n\nCoordination through shared symbolic communication is fundamental to human society, enabling us to collectively achieve goals beyond individual capabilities [ 1 ] .\nAs environments increasingly integrate artificial systems with humans, a critical challenge remains unsolved: how to enable distributed multi-agent systems to simultaneously develop shared symbol systems and effective coordination without centralized control. This paper addresses this challenge by proposing an approach that integrates world models with communication channels, enabling agent groups to form symbol systems while coordinating through communication in partially observable environments. Our method allows agents to use world models to estimate environmental states from partial observations, share this information through emergent communication via bidirectional message exchange using the acquired common symbol system, and achieve coordination in complex, partially observable settings.\n\nCoordination in multi-agent systems requires a shared symbol system where all participants interpret symbols consistently [ 2 ] .\nCommunication plays a crucial role by allowing agents to exchange independently acquired information and infer environmental states [ 3 ] , enabling them to complement each other‚Äôs knowledge and abilities, especially in partially observable environments [ 2 , 4 ] .\n\n Figure 1 : Overview of the proposed method. Each agent perceives a partial region of the environment, while complementing knowledge of other regions through communication. This leads to the emergence of a symbol system that represents collective knowledge. \n\nCurrent emergent communication approaches fail to address the dual challenge of symbol system learning and coordination in dynamic environments. Research in emergent communication has primarily focused on language development through games like Signaling Games and Referential Games [ 4 , 5 ] .\nHowever, these studies are typically limited to one-way communication and do not address multi-step action determination necessary for coordination in dynamic environments.\n\nWorld models-based approaches involve agents maintaining internal models that learn environmental dynamics and state estimation, forming latent state space representations from raw sensorimotor information [ 6 , 7 , 8 ] . These learned representations enable agents to predict future states and infer hidden environmental information from incomplete observations, potentially enhancing coordination capabilities in multi-agent settings [ 9 ] .\nHowever, current world model approaches in multi-agent settings lack mechanisms for collective knowledge formation through distributed symbol systems.\nTraditional multi-agent reinforcement learning methods often employ parameter sharing or centralized learning [ 10 , 11 ] , becoming computationally expensive and impractical in real-world settings [ 12 ] .\n\nDecentralized approaches to multi-agent coordination lack mechanisms for developing shared symbol systems.\nRecent research has explored decentralized approaches that transmit information through non-differentiable messages [ 13 , 14 , 15 ] , making them suitable for real-world applications where centralized control is impractical [ 16 ] .\nHowever, existing decentralized methods typically focus on immediate task achievement rather than developing shared symbol systems that capture environmental dynamics.\nThis limitation restricts their ability to form a common understanding that would enable more sophisticated coordination in complex, changing scenarios.\n\nExisting symbol emergence methods based on Collective Predictive Coding (CPC) [ 17 , 18 ] are limited to static observations and do not capture environmental dynamics. CPC extends predictive coding [ 19 ] and the free energy principle (FEP) [ 20 ] to social domains, viewing language and symbols as collective knowledge formed through the distributed participation of individual agents . Various CPC-based approaches have been developed, including methods based on Markov Chain Monte Carlo [ 21 , 22 ] and contrastive learning [ 23 ] . Some research has applied these concepts to multi-agent coordination by inferring symbols for multi-step action selection [ 24 , 25 ] . However, existing methods primarily focus on forming symbol systems from static observations at specific moments, making them inadequate for environments with continuous changes and temporal dependencies.\n\nOur proposed approach integrates world models with communication channels to enable simultaneous symbol system formation and coordination in dynamic environments. As illustrated in Figure 1 , we integrate world models with communication channels, allowing agents to predict environmental dynamics and share critical information through emergent communication. By reinterpreting FEP-based formulation of CPC [ 26 ] in a fully distributed form, we develop a system where agents learn to communicate without centralized control. Our contrastive learning mechanism aligns messages across agents, creating a unified symbol system that emerges naturally through learning.\n\nThis work makes three significant contributions to multi-agent coordination and communication research. First, we implement a two-agent world model that integrates CPC-based symbol emergence with temporal dynamics learning in a fully decentralized manner. Second, we demonstrate that environment-general symbols learned through world model prediction‚Äîrather than task-specific communication protocols‚Äîcan effectively support coordination in partially observable environments, allowing agents to complement each other‚Äôs limited perceptions. Third, we show that the distributed constraints of our approach lead to the formation of meaningful symbol systems that represent the global environmental state, emerging naturally from predictive learning of environmental dynamics. From the perspective of CPC research, our method extends previous CPC-based approaches temporally, providing a framework that captures environmental dynamics rather than static observations.\nWhile our approach shares similarities with recent work by [ 27 ] in using contrastive learning for message alignment, we uniquely develop environment-general symbolic representations through predictive modeling.\n\n II Formalization of Multi-Agent Interaction \n in Partially Observable Environments \n\nIn single-agent scenarios, the interaction between an agent and its environment is classically formalized as a Markov Decision Process (MDP). However, real-world agents rarely observe environmental states directly, instead perceiving their surroundings through limited sensory observations. To account for this limitation, world models and FEP approaches model such interactions as Partially Observable Markov Decision Processes (POMDPs).\n\nWhen multiple agents interact within a shared environment, the dynamics become more complex. For fully observable multi-agent settings, the interaction is formalized as a Markov Game [ 28 ] . A Markov Game with K K agents consists of a tuple ‚ü® ùí¶ , ùíÆ , { ùíú k } k , P ‚ü© \\langle\\mathcal{K},\\mathcal{S},\\{\\mathcal{A}^{k}\\}_{k},P\\rangle , where ùí¶ \\mathcal{K} represents the set of agents, ùíÆ \\mathcal{S} denotes the state space, and ùíú k \\mathcal{A}^{k} specifies the action space for agent k k . The transition function P ‚Äã ( s t + 1 ‚à£ s t , { a t k } k ) P(s_{t+1}\\mid s_{t},\\{a^{k}_{t}\\}_{k}) determines state transitions based on the current state s t s_{t} and actions a t k ‚àà ùíú k a^{k}_{t}\\in\\mathcal{A}^{k} taken by all agents.\n\nIn decentralized systems, agents operate autonomously without centralized control, making environmental state information inaccessible to individual agents. Moreover, each agent receives distinct observations based on its unique perspective. Such partially observable, distributed settings are formalized through Decentralized Partially Observable MDPs (Dec-POMDPs) [ 29 ] .\nA Dec-POMDP is structured as a tuple ‚ü® ùí¶ , ùíÆ , { ùíú k } k , P , { ùí™ k } k , { Œ© k } k ‚ü© \\langle\\mathcal{K},\\mathcal{S},\\{\\mathcal{A}^{k}\\}_{k},P,\\{\\mathcal{O}^{k}\\}_{k},\\{\\Omega^{k}\\}_{k}\\rangle , where ùí™ k \\mathcal{O}^{k} represents agent k k ‚Äôs private observation space, and Œ© k ‚Äã ( o t k ‚à£ s t ) \\Omega^{k}(o^{k}_{t}\\mid s_{t}) denotes the observation function governing how agent k k perceives state s t s_{t} .\n\nOur approach aims to model cooperative agent groups within Dec-POMDP environments, where a fundamental challenge emerges: environmental states evolve as a function of all agents‚Äô collective actions. This collective influence means that each agent‚Äôs observations are affected not only by its own actions but also by those of other agents in the system. Consequently, from any individual agent‚Äôs perspective, the environment exhibits non-stationary characteristics, which significantly complicates predictive modeling.\n\nThe partial observability inherent to Dec-POMDP presents an additional challenge for optimal decision-making. When an agent‚Äôs sensory input captures only a subset of the environmental state, its ability to comprehend the global environment is fundamentally limited.\n\nTo address this limitation, we employ CPC, which introduces shared symbolic representations functioning as distributed knowledge across all agents. This framework enables the integration of diverse agent perspectives through message exchange.\nOur method leverages this emergent property of CPC to enable coordinated action determination by synthesizing: (1) partial observation obtained through interaction with environment, and (2) messages from other agents. This dual-channel approach enhances decision-making quality while maintaining the decentralized nature of the system.\n\n III Proposed model \n\nIn this section, we first describe the centralized model architecture and objective function for our two-agent system. Next, we explain how this centralized formulation is decomposed for decentralized implementation through approximation methods. Finally, we detail the action determination procedure of agents.\n\n III-A CPC-based Multi-agent World Model \n\n Figure 2 : The architecture of the collective world model. Bidirectional arrows indicate losses for training. \n\nFollowing the CPC framework, we first formulate a centralized model that assumes access to shared messages between agents, then decompose it for decentralized implementation. Our proposed model, illustrated in Figure 2 , is constructed around interconnected agent world models that jointly infer messages through exchange.\nEach world model learns environmental dynamics as state transitions in a latent space by modeling the observation generation process based on a POMDP. We employ the Recurrent State Space Model [ 30 ] as the foundational architecture for world models.\nOur key innovation is the introduction of a probabilistic variable m t m_{t} representing messages exchanged between agents, which follows a continuous distribution enabling effectively infinite possible messages, and influences the generation of each agent‚Äôs latent variables at each time step.\nFor a system with two agents ( A , B \\mathrm{A},\\mathrm{B} ), the multi-agent world model comprises the following components:\n\n Message inference model: q ‚Äã ( m t ‚à£ s t A , s t B ) , Representation model: ‚Äã q ‚Äã ( s t ‚àó ‚à£ s t ‚àí 1 ‚àó , m t ‚àí 1 , a t ‚àí 1 ‚àó , o t ‚àó ) , Message generation model: p ‚Äã ( m t ) , Observation model: p ‚Äã ( o t ‚àó ‚à£ s t ‚àó ) , Transition model: p ‚Äã ( s t ‚àó ‚à£ s t ‚àí 1 ‚àó , m t ‚àí 1 , a t ‚àí 1 ‚àó ) , for ‚àó ‚àà { A , B } . \\displaystyle\\begin{split}&amp;\\text{Message inference model:}\\quad\\quad q(m_{t}\\mid s_{t}^{\\mathrm{A}},s_{t}^{\\mathrm{B}}),\\\\\n&amp;\\text{Representation model:}~q(s_{t}^{*}\\mid s_{t-1}^{*},m_{t-1},a_{t-1}^{*},o_{t}^{*}),\\\\\n&amp;\\text{Message generation model:}\\quad p(m_{t}),\\\\\n&amp;\\text{Observation model:}\\quad\\quad\\quad\\quad p(o_{t}^{*}\\mid s_{t}^{*}),\\\\\n&amp;\\text{Transition model:}\\quad p(s_{t}^{*}\\mid s_{t-1}^{*},m_{t-1},a_{t-1}^{*}),\\\\\n&amp;\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\mathrm{for}~*\\in\\{\\mathrm{A},\\mathrm{B}\\}.\\end{split} \n\n (1) \n\nHere, o t ‚àó , a t ‚àó o_{t}^{*},~a_{t}^{*} , and s t ‚àó s_{t}^{*} represent the observation, action, and latent variable of agent ‚àó * at time t t , respectively. The latent state s t ‚àó s^{*}_{t} encompasses both a probabilistic latent variable z t ‚àó z^{*}_{t} and a deterministic latent variable h t ‚àó h^{*}_{t} , where h t ‚àó h_{t}^{*} is the internal state of the Gated Recurrent Unit [ 31 ] in agent ‚àó * ‚Äôs world model. Additionally, m t m_{t} denotes a probabilistic variable representing the common message shared between agents at time t t .\n\nThe multi-agent world model is trained by minimizing the variational free energy (VFE), which establishes an upper bound on the negative log-likelihood. To distinguish between the VFE of the entire group and that of each individual agent, we designate the former as the collective free energy (CFE). The CFE is formulated as:\n\n F \\displaystyle F \n = ‚àë t = 1 T [ ‚àë ‚àó ‚Å£ ‚àà { A , B } { ‚àí ùîº q [ log p ( o t ‚àó ‚à£ s t ‚àó ) ] \\displaystyle=\\sum_{t=1}^{T}\\Bigg[\\sum_{*\\in\\{\\mathrm{A},\\mathrm{B}\\}}\\bigg\\{-\\mathbb{E}_{q}\\left[\\log p({o}_{t}^{*}\\mid{s}_{t}^{*})\\right] \n\n + D K ‚Äã L [ q ( s t ‚àó ‚à£ s t ‚àí 1 ‚àó , m t ‚àí 1 , a t ‚àí 1 ‚àó , o t ‚àó ) | | \\displaystyle\\quad+D_{KL}\\Big[q(s_{t}^{*}\\mid s_{t-1}^{*},m_{t-1},a_{t-1}^{*},o_{t}^{*})|| \n\n p ( s t ‚àó ‚à£ s t ‚àí 1 ‚àó , m t ‚àí 1 , a t ‚àí 1 ‚àó ) ] } \\displaystyle\\quad\\quad\\quad\\quad\\quad p(s_{t}^{*}\\mid s_{t-1}^{*},m_{t-1},a_{t-1}^{*})\\Big]\\bigg\\} \n\n + D K ‚Äã L [ q ( m t ‚àí 1 ‚à£ s t ‚àí 1 A , s t ‚àí 1 B ) | | p ( m t ‚àí 1 ) ] ] \\displaystyle\\quad+D_{KL}\\left[q(m_{t-1}\\mid s_{t-1}^{\\mathrm{A}},s_{t-1}^{\\mathrm{B}})||p(m_{t-1})\\right]\\Bigg] \n\n = ‚àë t = 1 T [ ‚àë ‚àó ‚Å£ ‚àà { A , B } ‚Ñí reconst ‚àó t + ‚Ñí KLD ‚àó t ‚èü Individual VFE \\displaystyle=\\sum_{t=1}^{T}\\Bigg[\\sum_{*\\in\\{\\mathrm{A},\\mathrm{B}\\}}\\underbrace{{\\mathcal{L}^{*}_{\\mathrm{reconst}}}_{t}+{\\mathcal{L}^{*}_{\\mathrm{KLD}}}_{t}}_{\\text{Individual VFE}} \n\n + D K ‚Äã L [ q ( m t ‚àí 1 ‚à£ s t ‚àí 1 A , s t ‚àí 1 B ) | | p ( m t ‚àí 1 ) ] ‚èü Collective regularization term ] . \\displaystyle\\quad+\\underbrace{D_{KL}\\left[q(m_{t-1}\\mid s_{t-1}^{\\mathrm{A}},s_{t-1}^{\\mathrm{B}})||p(m_{t-1})\\right]}_{\\text{Collective regularization term}}\\Bigg]. \n\n (2) \n\nThe first and second terms represent the individual VFE and have the effect of forming internal representations. The third term is the collective regularization (CR) term, which promotes message formation [ 26 ] .\n\n III-B Approximation of the CR term for Distributed Learning \n\nA significant challenge emerges in implementing the aforementioned model. While our theoretical formulation presupposes the existence of a model q ‚Äã ( m t ‚à£ s t A , s t B ) q(m_{t}\\mid s_{t}^{\\mathrm{A}},s_{t}^{\\mathrm{B}}) that infers messages from the latent variables s t ‚àó s_{t}^{*} of two agents, such a model cannot be directly computed in a decentralized system. In natural symbol emergence, an individual‚Äôs cognitive system is confined to its sensorimotor boundaries, rendering it impossible for an agent to directly access other agents‚Äô internal representations. This constraint makes the CR term incalculable in its original form, presenting a substantial impediment to implementing distributed learning across agents.\n\nTo address this constraint, we propose an approximation methodology that enables independent learning for each agent. We introduce independent probabilistic variables m t A m^{\\mathrm{A}}_{t} and m t B m^{\\mathrm{B}}_{t} representing each agent‚Äôs estimate of the shared message m t m_{t} . We then approximate both the prior distribution p ‚Äã ( m t ) p(m_{t}) and posterior distribution q ‚Äã ( m t ‚à£ s t A , s t B ) q(m_{t}\\mid s_{t}^{\\mathrm{A}},s_{t}^{\\mathrm{B}}) of the message using a Product-of-Experts (PoE) formulation:\n\n p ‚Äã ( m t ) \\displaystyle p(m_{t}) \n ‚âà C pm ‚Äã ‚àè ‚àó ‚Å£ ‚àà { A , B } p ‚Äã ( m t ‚àó ) , \\displaystyle\\approx C_{\\text{pm}}\\prod_{*\\in\\{\\mathrm{A},\\mathrm{B}\\}}p(m^{*}_{t}), \n\n (3) \n\n q ‚Äã ( m t ‚à£ s t A , s t B ) \\displaystyle q(m_{t}\\mid s_{t}^{\\mathrm{A}},s_{t}^{\\mathrm{B}}) \n ‚âà C qm ‚Äã ‚àè ‚àó ‚Å£ ‚àà { A , B } q ‚Äã ( m t ‚àó ‚à£ s t ‚àó ) \\displaystyle\\approx C_{\\text{qm}}\\prod_{*\\in\\{\\mathrm{A},\\mathrm{B}\\}}q(m^{*}_{t}\\mid s_{t}^{*}) \n\n (4) \n\nwhere C pm C_{\\text{pm}} and C qm C_{\\text{qm}} denote normalization constants.\nThis approximation is theoretically justified for both posterior and prior distributions in our context. For posterior distributions, this approach aligns with realistic constraints where agents can only infer messages based on their own internal representations‚Äîprecisely matching the decentralized nature of the learning problem. For prior distributions, the approximation is appropriate when interpreting collective prior knowledge about symbols as an integration of individual agents‚Äô knowledge bases. This formulation not only maintains computational tractability but also naturally promotes consensus in message representation, which is fundamental for symbol emergence in multi-agent systems.\n\nWith this approximation established, we must determine how to set the prior distribution of messages p ‚Äã ( m t ‚àó ) p(m^{*}_{t}) for each agent. We propose defining p ‚Äã ( m t ‚àó ) p(m^{*}_{t}) as a distribution that reflects the collective prior knowledge at that time. Specifically, we consider a distribution based on what messages the other agent is inferring at that moment. Thus, we define each agent‚Äôs prior distribution as the other agent‚Äôs posterior distribution:\n\n p ‚Äã ( m t A ) ‚âú q ‚Äã ( m t B ‚à£ s t B ) , p ‚Äã ( m t B ) \\displaystyle p(m^{\\mathrm{A}}_{t})\\triangleq q(m^{\\mathrm{B}}_{t}\\mid{s}_{t}^{\\mathrm{B}}),\\quad p(m^{\\mathrm{B}}_{t}) \n ‚âú q ‚Äã ( m t A ‚à£ s t A ) . \\displaystyle\\triangleq q(m^{\\mathrm{A}}_{t}\\mid{s}_{t}^{\\mathrm{A}}). \n\n (5) \n\nThis formulation establishes a feedback mechanism wherein each agent‚Äôs message inference is influenced by the collective understanding of the other agent, facilitating convergence to a shared communication protocol.\n\nWe designate the agent that infers messages from observations and transmits them to the other agent as the Speaker, and the agent that updates parameters based on observations and received messages from the other agent as the Listener. The Listener‚Äôs CR term at time t t is the KL divergence related to both agents‚Äô message posterior distributions:\n\n D K ‚Äã L [ q ( m t Sp ‚à£ s t Sp ) | | p ( m t Sp ) ] \\displaystyle D_{KL}\\left[q(m^{\\mathrm{Sp}}_{t}\\mid{s}_{t}^{\\mathrm{Sp}})||p(m^{\\mathrm{Sp}}_{t})\\right] \n\n = D K ‚Äã L [ q ( m t Sp ‚à£ s t Sp ) | | q ( m t Li ‚à£ s t Li ) ] \\displaystyle\\quad=D_{KL}\\left[q(m^{\\mathrm{Sp}}_{t}\\mid{s}_{t}^{\\mathrm{Sp}})||q(m^{\\mathrm{Li}}_{t}\\mid{s}_{t}^{\\mathrm{Li}})\\right] \n\n (6) \n\nwhere s t Li , m t Li s^{\\mathrm{Li}}_{t},~m^{\\mathrm{Li}}_{t} and s t Sp , m t Sp s^{\\mathrm{Sp}}_{t},~m^{\\mathrm{Sp}}_{t} represent the latent variables and messages of the Listener and Speaker, respectively. A practical challenge arises in calculating equation ( III-B ), as it requires the parameters of the message posterior distributions. However, in realistic scenarios, agents communicate by exchanging sampled messages rather than distributional parameters. Each agent samples messages from its posterior distribution and exchanges these samples with other agents. Consequently, an agent has access only to message samples from other agents, not to the parameters of their posterior distributions. To address this limitation, we leverage the Noise Contrastive Estimation framework [ 32 ] and employ the InfoNCE loss [ 33 ] to estimate the message inference model from samples alone:\n\n ‚Ñí NCE Li t \\displaystyle{\\mathcal{L}^{\\mathrm{Li}}_{\\mathrm{NCE}}}_{t} \n\n = ‚àí ùîº q ‚Äã ( m t Sp ‚à£ s t Sp ) , q ‚Äã ( m t Li ‚à£ s t Li ) ‚Äã [ log ‚Å° sim ‚Äã ( m t Li , m t Sp ) ùîº q ‚Äã ( m ‚Ä≤ t Sp ‚à£ s t ‚Ä≤ Sp ) ‚Äã [ sim ‚Äã ( m t Li , m ‚Ä≤ t Sp ) ] ] . \\displaystyle=-\\mathbb{E}_{\\begin{subarray}{c}q(m^{\\mathrm{Sp}}_{t}\\mid{s}_{t}^{\\mathrm{Sp}}),\\\\\nq(m^{\\mathrm{Li}}_{t}\\mid{s}_{t}^{\\mathrm{Li}})\\end{subarray}}\\left[\\log\\frac{\\mathrm{sim}(m^{\\mathrm{Li}}_{t},~m^{\\mathrm{Sp}}_{t})}{\\mathbb{E}_{q({m^{\\prime}}^{\\mathrm{Sp}}_{t}\\mid{s^{\\prime}_{t}}^{\\mathrm{Sp}})}\\left[\\mathrm{sim}(m^{\\mathrm{Li}}_{t},~{m^{\\prime}}^{\\mathrm{Sp}}_{t})\\right]}\\right]. \n\n (7) \n\nIn practice, each agent ‚àó ‚àà { A , B } *\\in\\{\\mathrm{A},\\mathrm{B}\\} learns by minimizing the weighted distributed CFE F ‚àó F^{*} :\n\n F ‚àó \\displaystyle F^{*} \n = ‚àë t = 1 T ( ‚Ñí reconst ‚àó t + w KLD ‚Äã ‚Ñí KLD ‚àó t + w NCE ‚Äã ‚Ñí NCE ‚àó t ) . \\displaystyle=\\sum_{t=1}^{T}({\\mathcal{L}^{*}_{\\mathrm{reconst}}}_{t}+w_{\\mathrm{KLD}}{\\mathcal{L}^{*}_{\\mathrm{KLD}}}_{t}+w_{\\mathrm{NCE}}{\\mathcal{L}^{*}_{\\mathrm{NCE}}}_{t}). \n\n (8) \n\nSince F ‚àó F^{*} can be computed independently for agents A \\mathrm{A} and B \\mathrm{B} , we optimize F A F^{\\mathrm{A}} and F B F^{\\mathrm{B}} simultaneously during training, enabling decentralized multi-agent world model learning.\n\n III-C Action Determination through Communication \n\nIn Dec-POMDPs, effective cooperation requires agents to infer the complete environmental state despite having only partial observations. This limitation highlights the critical importance of information exchange among agents to achieve comprehensive environmental understanding.\n\nAgents generate actions by inferring internal representations and messages based on observation history. To mitigate computational challenges with increasing temporal horizons, we implement a sliding time window of length W W , wherein information beyond W W timesteps is disregarded. Each agent maintains queues of maximum length W + 1 W+1 for observations and W W for past actions.\n\n Algorithm 1 Action Determination \n\n 1: procedure Action Determination ( o t ‚àí W : t A o^{\\mathrm{A}}_{t-W:t} , a t ‚àí W : t ‚àí 1 A a^{\\mathrm{A}}_{t-W:t-1} , s t ‚àí W A s^{\\mathrm{A}}_{t-W} , o t ‚àí W : t B o^{\\mathrm{B}}_{t-W:t} , a t ‚àí W : t ‚àí 1 B a^{\\mathrm{B}}_{t-W:t-1} , s t ‚àí W B s^{\\mathrm{B}}_{t-W} )\n\n 2: ‚ÄÉ‚ÄÇ m t ‚àí W : t ‚àí 1 A ‚àº q ( ‚ãÖ ‚à£ s t ‚àí W A , a t ‚àí W : t ‚àí 1 A , o t ‚àí W : t A ) m^{\\mathrm{A}}_{t-W:t-1}\\sim q(\\cdot\\mid s^{\\mathrm{A}}_{t-W},a^{\\mathrm{A}}_{t-W:t-1},o^{\\mathrm{A}}_{t-W:t}) \n\n 3: ‚ÄÉ‚ÄÇ m t ‚àí W : t ‚àí 1 B ‚àº q ( ‚ãÖ ‚à£ s t ‚àí W B , a t ‚àí W : t ‚àí 1 B , o t ‚àí W : t B ) m^{\\mathrm{B}}_{t-W:t-1}\\sim q(\\cdot\\mid s^{\\mathrm{B}}_{t-W},a^{\\mathrm{B}}_{t-W:t-1},o^{\\mathrm{B}}_{t-W:t}) ‚ä≥ \\triangleright (1)\n\n 4: ‚ÄÉ‚ÄÇ if ‚Ñí VFE A ‚Äã ( m t ‚àí W : t ‚àí 1 A ) ‚â§ ‚Ñí VFE A ‚Äã ( m t ‚àí W : t ‚àí 1 B ) \\mathcal{L}^{\\mathrm{A}}_{\\text{VFE}}(m^{\\mathrm{A}}_{t-W:t-1})\\leq\\mathcal{L}^{\\mathrm{A}}_{\\text{VFE}}(m^{\\mathrm{B}}_{t-W:t-1}) then ‚ä≥ \\triangleright (2,3)\n\n 5: ‚ÄÉ‚ÄÉ‚ÄÉ a ^ t A ‚Üê œÄ A ‚Äã ( s t A , m t ‚àí 1 A ) \\hat{a}^{\\mathrm{A}}_{t}\\leftarrow\\pi^{\\mathrm{A}}(s^{\\mathrm{A}}_{t},m^{\\mathrm{A}}_{t-1}) \n\n 6: ‚ÄÉ‚ÄÇ else \n\n 7: ‚ÄÉ‚ÄÉ‚ÄÉ a ^ t A ‚Üê œÄ A ‚Äã ( s t A , m t ‚àí 1 B ) \\hat{a}^{\\mathrm{A}}_{t}\\leftarrow\\pi^{\\mathrm{A}}(s^{\\mathrm{A}}_{t},m^{\\mathrm{B}}_{t-1}) ‚ä≥ \\triangleright (4)\n\n 8: ‚ÄÉ‚ÄÇ end if \n\n 9: ‚ÄÉ‚ÄÇ if ‚Ñí VFE B ‚Äã ( m t ‚àí W : t ‚àí 1 B ) ‚â§ ‚Ñí VFE B ‚Äã ( m t ‚àí W : t ‚àí 1 A ) \\mathcal{L}^{\\mathrm{B}}_{\\text{VFE}}(m^{\\mathrm{B}}_{t-W:t-1})\\leq\\mathcal{L}^{\\mathrm{B}}_{\\text{VFE}}(m^{\\mathrm{A}}_{t-W:t-1}) then ‚ä≥ \\triangleright (2,3)\n\n 10: ‚ÄÉ‚ÄÉ‚ÄÉ a ^ t B ‚Üê œÄ B ‚Äã ( s t B , m t ‚àí 1 B ) \\hat{a}^{\\mathrm{B}}_{t}\\leftarrow\\pi^{\\mathrm{B}}(s^{\\mathrm{B}}_{t},m^{\\mathrm{B}}_{t-1}) \n\n 11: ‚ÄÉ‚ÄÇ else \n\n 12: ‚ÄÉ‚ÄÉ‚ÄÉ a ^ t B ‚Üê œÄ B ‚Äã ( s t B , m t ‚àí 1 A ) \\hat{a}^{\\mathrm{B}}_{t}\\leftarrow\\pi^{\\mathrm{B}}(s^{\\mathrm{B}}_{t},m^{\\mathrm{A}}_{t-1}) ‚ä≥ \\triangleright (4)\n\n 13: ‚ÄÉ‚ÄÇ end if \n\n 14: ‚ÄÉ‚ÄÇ return ( a ^ t A , a ^ t B ) (\\hat{a}^{\\mathrm{A}}_{t},\\hat{a}^{\\mathrm{B}}_{t}) \n\n 15: end procedure \n\nWhen determining actions, agents infer internal representations and messages using the information stored in their respective queues. These internal representations are influenced not only by the agent‚Äôs observations and actions but also by messages received in the previous timestep. For optimal coordination, agents must utilize messages that maximize observation predictability.\nOur model enables agents to infer messages within a shared representational space and compare self-generated messages with those received from others. Each agent selects the message that minimizes the VFE, thereby inferring representations that best explain past observations.\nEach agent determines actions according to the following steps (Algorithm 1 ):\n\n 1. \n\n Inference of Internal Representations and Messages :\nAgents receive observations o t ‚àó o^{*}_{t} from the environment, add them to their queues, and perform sequential inference of internal representations and messages based on their observation-action sequences.\n\n s t ‚àí œÑ ‚àó ‚àº q ‚àó ( ‚ãÖ | s t ‚àí œÑ ‚àí 1 ‚àó , m t ‚àí œÑ ‚àí 1 ‚àó , a t ‚àí œÑ ‚àí 1 ‚àó , o t ‚àí œÑ ‚àó ) , \\displaystyle s^{*}_{t-\\tau}\\sim q^{*}(\\cdot|s^{*}_{t-\\tau-1},m^{*}_{t-\\tau-1},a^{*}_{t-\\tau-1},o^{*}_{t-\\tau}), \n\n (9) \n\n m t ‚àí œÑ ‚àó ‚àº q ‚àó ( ‚ãÖ | s t ‚àí œÑ ‚àó ) for œÑ = W , ‚Ä¶ , 1 . \\displaystyle m^{*}_{t-\\tau}\\sim q^{*}(\\cdot|s^{*}_{t-\\tau})\\quad\\textrm{for}~\\tau=W,\\ldots,1. \n\n (10) \n\n 2. \n\n Message Exchange : Both agents reciprocally transmit their inferred message sequences m t ‚àí W : t ‚àí 1 ‚àó m^{*}_{t-W:t-1} that were derived in step 1.\n\n 3. \n\n Selection of Messages and Internal Representations :\nEach agent reconstructs observations using both self-generated and received messages, computes the individual VFE for each, and selects the message sequence yielding the minimum VFE.\n\n 4. \n\n Action Determination :\nEach agent determines its action using its current internal representation s t ‚àó s^{*}_{t} and the terminal message m t ‚àí 1 ‚àó m^{*}_{t-1} from the selected message sequence. The determined action is then pushed into the agent‚Äôs action queue.\n\n IV experiment \n\n IV-A Task Setup \n\nWe conducted experiments in a simulated environment to investigate whether multiple agents can achieve coordinated behavior through message formation that represents the complete environmental state.\nThe experimental task involved two agents collaboratively moving a point P P in two-dimensional space to trace a predefined trajectory.\nEach agent receives sensory signals from point P coordinates through agent-specific sensory modules.\nThese modules discretize one coordinate axis into a predetermined number of bins and introduce noise. Specifically, agent A‚Äôs sensory module discretizes the y-axis coordinates, while agent B‚Äôs module discretizes the x-axis coordinates.\nBy manipulating the number of bins in these sensory modules, we can systematically control the extent of environmental information accessible to each agent. With an infinite number of bins, no discretization occurs, enabling complete environmental perception. Conversely, with a single bin (bin=1), agents can observe only one axis, effectively reducing each agent‚Äôs perceptible environmental state by half. This configuration allows us to transition from a Markov Game setting (infinite bins) to a Decentralized Partially Observable Markov Decision Process (Dec-POMDP) setting (finite bins).\n\n (a) \n\n (b) \n\n Figure 3 : (a) Schematic overview of the trajectory drawing coordination task environment created for the experiment. (b) The hypotrochoid trajectory that agents were required to draw in this experiment. \n\nEach agent controls the velocity along one of the axes in a coordinate system derived by rotating the x-y coordinate system by ‚àí œÄ / 4 -\\pi/4 . This rotation ensures that the fully observable axis coordinates are affected by both agents‚Äô actions. Figure 3 illustrates the environmental configuration, depicting both observation and action axes.\nWe implemented a task requiring agents to draw a star-shaped hypotrochoid curve.\n\nWe established five distinct configurations for the number of bins in each agent‚Äôs sensory module: infinite, 8, 6, 2, and 1. For each configuration, we generated 2000 samples of coordinated expert data, with each sample completing one circuit of the trajectory in 200 steps.\nWe examine how the agents‚Äô capacity to develop effective symbolic representations of the environment influences their coordination capabilities, particularly as their individual observational capabilities become increasingly constrained.\n\n IV-B Model Architecture and Training Setup \n\nBoth agent models were constructed with identical architectures, although their parameters were independently initialized with different random values. To enhance learning stability, we implemented the distribution of the latent variable z t ‚àó z^{*}_{t} as a unimix categorical distribution [ 7 ] with a 1% uniform mixture component, utilizing a 4-dimensional representation with 4 distinct classes.\nFor the message distribution, we employed a two-dimensional multivariate Gaussian distribution. The internal state dimension of the GRU, h t ‚àó h^{*}_{t} , was set to 32. Within the CFE (equation ( 8 )), the weighting coefficients w KLD w_{\\textrm{KLD}} and w NCE w_{\\textrm{NCE}} were set to 0.01 and 0.005, respectively. For the InfoNCE loss, we utilized negative squared Euclidean distance as the similarity function, incorporating a temperature parameter œÑ \\tau set to 2.0:\n\n sim ‚Äã ( m , m ‚Ä≤ ) = ‚àí ‚Äñ m ‚àí m ‚Ä≤ ‚Äñ 2 2 œÑ . \\displaystyle\\textrm{sim}(m,m^{\\prime})=-\\frac{\\|m-m^{\\prime}\\|^{2}_{2}}{\\tau}. \n\n (11) \n\nIn this experiment, we trained the policy through behavioral cloning of expert demonstrations. Specifically, we defined the policy learning objective as the Mean Squared Error between the predicted actions and the expert actions:\n\n ‚Ñí œÄ ‚àó = ‚àë t ‚Äñ œÄ ‚àó ‚Äã ( s t ‚àó , m t ‚àí 1 ‚àó ) ‚àí a t ‚àó ‚Äñ 2 2 . \\displaystyle\\mathcal{L}^{*}_{\\pi}=\\sum_{t}\\|\\pi^{*}(s^{*}_{t},m^{*}_{t-1})-a^{*}_{t}\\|^{2}_{2}. \n\n (12) \n\nAll models were trained using a batch size of 500 for 1,000 epochs to ensure convergence of the learning process.\n\n IV-C Conditions \n\nTo systematically investigate the effects of decentralization and symbol emergence through the introduction of InfoNCE loss on world model learning and cooperative behavior acquisition, we established three experimental conditions:\n\n ‚Ä¢ \n\n EmergentCommunication (EC): Our proposed method, in which agents develop symbol systems with InfoNCE loss while constructing decentralized world models.\n\n ‚Ä¢ \n\n BrainConnected (BC): A condition without InfoNCE loss and without model decentralization. In this setup, message inference distribution is conditioned on both agents‚Äô internal states, wherein the message-inferring model incorporates both agents‚Äô latent states s t A , s t B s^{A}_{t},~s^{B}_{t} as inputs. The loss function comprises the sum of individual VFEs for each agent.\n\n ‚Ä¢ \n\n NoCommunication (NC): A condition where both agents maintain and train independent world models. The internal representations s t ‚àó s^{*}_{t} and messages m t ‚àó m^{*}_{t} of each agent remain independent, with each agent optimizing its Individual VFE as the loss function.\n\nAdditionally, we established a control condition featuring a single agent with complete environmental observability as a performance baseline.\n\nFor all experimental conditions, we conducted training and evaluation across three distinct random seeds. For each seed, we generated 100 test samples for comprehensive evaluation. The time window parameter W W used for EC evaluation was fixed at 10 throughout our experiments.\n\n V Result and Discussion \n\n V-A The Effectiveness of Decentralized Symbol Emergence \n\n Figure 4 : Comparison of coordination achievement across learning conditions. The values represent the average of maximum cross-correlation between trajectories drawn by agents and test data. Error bars represent standard deviation. \n\nWe compared coordination performance across learning conditions. Point P P was randomly positioned at the beginning of each trial, with 100 trials conducted using the learned policies to generate trajectories. Coordination success was quantified using the maximum cross-correlation between the generated trajectories and the ideal hypotrochoid trajectory.\n\nFigure 4 presents the mean scores for each condition and bin configuration. In the Markov Game condition (infinite number of bins), agents had complete access to the environmental state, enabling them to select optimal actions without requiring information exchange with other agents. This resulted in minimal performance differences between conditions, with all approaches achieving scores comparable to the baseline.\nConversely, in Dec-POMDP conditions (finite bin configurations), the BC condition demonstrated superior coordination, followed by the EC condition, while the NC condition exhibited the lowest performance. The enhanced performance of both EC and BC conditions, which leverage shared messages between agents, compared to the NC condition where agents learn independently, clearly demonstrates that communication channels built upon unified representational systems facilitate more effective coordination.\n\nExamining the performance gap between EC and NC conditions, we observe that as the number of observation bins decreases, this gap widens significantly. This finding suggests that communication through shared symbol systems becomes increasingly critical for successful coordination when agents experience greater disparities in information accessibility‚Äîsuch as when they operate at considerable physical distances or when they possess different observable information modalities.\n\n V-B The Effectiveness of Symbolic Communication \n\n Figure 5 : Comparison of coordination achievement with ( w/ com ) and without ( w/o com ) communication through message exchange using the EC (proposed method) model. Error bars represent standard deviation. \n\nWe investigated whether symbolic communication through message exchange, as described in Section III-C , enhances coordination success. We compared performance between two conditions: w/ com , which employs our proposed method with communication, and w/o com , where agents make decisions using only self-inferred messages without communication.\n\nFigure 5 illustrates these results. For conditions with bin numbers exceeding 1, w/ com exhibited higher performance than w/o com in most cases, although with substantial overlap in standard deviation ranges, indicating minimal differences.\nNotably, when the bin number equals 1‚Äîrepresenting scenarios where observable information differs completely between agents‚Äî w/ com consistently outperformed w/o com .\nThese results provide compelling evidence that symbolic communication offers particular advantages when agents possess heterogeneous perceptual capabilities.\n\n V-C Analysis of Emergent Messages \n\nWe examined how effectively the message structure formed through learning reflected the environmental state. Using Representational Similarity Analysis (RSA) [ 34 ] , we evaluated the structural similarity between messages inferred during test data reconstruction and the trajectory of point P P .\nFor each episode, we calculated distances between data points at all possible time step combinations for both inferred messages and actual point P P coordinates, creating dissimilarity matrices. We then extracted the upper triangular components (excluding diagonal elements) and calculated Spearman‚Äôs rank correlation. Higher scores indicate greater structural similarity between the point P P trajectory and the sequence of inferred messages, demonstrating that the model‚Äôs acquired messages more accurately represent the environmental state. This metric corresponds to the concept of Positive Signaling [ 35 ] , which measures message effectiveness between agents by quantifying the correlation between an agent‚Äôs inferred messages and its observations.\n\n Figure 6 : Similarity between the structure of inferred messages when reconstructing test data observations and the structure of the actual trajectory of point P P , as calculated by RSA. Error bars represent standard deviation. \n\n Figure 7 : (Left) Trajectory of point P P when moved according to test data, and (Right) sequence of messages inferred by each agent when reconstructing observations using EC (proposed method) with 6 bins. In all plots, the color of points changes from blue to red as time steps progress. \n\nFigure 6 presents a comparative analysis of scores across learning conditions. It should be noted that for the BC condition, only one message is inferred since the models of each agent are connected through message variables. Across all conditions, messages acquired through EC demonstrated superior environmental state representation compared to other conditions. Significantly, EC outperformed BC despite the latter allowing agents direct access to other agents‚Äô internal states. This finding suggests that in CPC, distributed constraints‚Äîwhere agents cannot directly access other agents‚Äô internal states or observations‚Äîactually facilitate the emergence of more meaningful symbol systems.\n\nFigure 7 illustrates the trajectory of inferred messages in the EC, showcasing a trial example with 6 bins. Comparison with the actual trajectory of point P P reveals that the actual environmental state is effectively captured in the message representation space, despite each agent receiving only partial observations.\nFurthermore, due to the alignment effect of InfoNCE loss, nearly identical messages are inferred by different agents at each time step, despite variations in their observations and actions.\n\nAlthough EC acquired meaningful message representations, its coordination performance remained below that of BC as previously noted. This discrepancy likely stems from limitations in our policy learning methodology, where the representations acquired by world models were not optimally utilized by the policies. Our experiments employed only imitation learning of expert actions without incorporating exploratory behaviors or reinforcement learning. Without experiencing coordination failures during training, agents lacked the critical negative feedback necessary to refine their coordination strategies, which likely contributed to the observed performance gap between EC and BC.\n\n VI conclusion \n\nWe proposed a method for multiple agents to acquire a common symbol system for mutual communication and achieve coordinative behavior in Dec-POMDP settings. Our multi-agent world model incorporates a communication channel enabling information exchange through a shared representational system. Based on CPC, we derived a learning rule operating in a completely distributed manner and proposed an action determination algorithm where agents interact through the acquired communication channel.\n\nWe designed a task requiring two agents to cooperatively trace a desired trajectory and compared our approach with centralized and non-communicative models. The results confirmed that symbol system formation and communication contribute to coordination, particularly when observable information differs between agents. We demonstrated that meaningful messages effectively reflecting the overall environmental state can emerge through distributed learning.\n\nFuture work includes incorporating reinforcement learning and active inference into policy learning, extending our approach to larger agent groups, and evaluating performance on more complex tasks with higher-dimensional observations. In this work, we focus on message emergence that supports cooperation, while exploration of environments by multi-agents and skill acquisition from scratch will be addressed in future work."
}