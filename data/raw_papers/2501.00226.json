{
  "arxiv_id": "2501.00226",
  "title": "Generative Emergent Communication: Large Language Model is a Collective World Model",
  "authors": [
    "Taniguchi, Tadahiro",
    "Ueda, Ryo",
    "Nakamura, Tomoaki",
    "Suzuki, Masahiro",
    "Taniguchi, Akira"
  ],
  "date": "2024/12/31",
  "abstract": "Large Language Models (LLMs) have demonstrated a remarkable ability to capture extensive world knowledge, yet how this is achieved without direct sensorimotor experience remains a fundamental puzzle. This study proposes a novel theoretical solution by introducing the Collective World Model hypothesis. We argue that an LLM does not learn a world model from scratch; instead, it learns a statistical approximation of a collective world model that is already implicitly encoded in human language through a society-wide process of embodied, interactive sense-making. To formalize this process, we introduce generative emergent communication (Generative EmCom), a framework built on the Collective Predictive Coding (CPC). This framework models the emergence of language as a process of decentralized Bayesian inference over the internal states of multiple agents. We argue that this process effectively creates an encoder-decoder structure at a societal scale: human society collectively encodes its grounded, internal representations into language, and an LLM subsequently decodes these symbols to reconstruct a latent space that mirrors the structure of the original collective representations. This perspective provides a principled, mathematical explanation for how LLMs acquire their capabilities. The main contributions of this paper are: 1) the formalization of the Generative EmCom framework, clarifying its connection to world models and multi-agent reinforcement learning, and 2) its application to interpret LLMs, explaining phenomena such as distributional semantics as a natural consequence of representation reconstruction. This work provides a unified theory that bridges individual cognitive development, collective language evolution, and the foundations of large-scale AI.",
  "origin": "https://arxiv.org/abs/",
  "key": "2501.00226",
  "body": "Generative Emergent Communication:\n Large Language Model is a Collective World Model \n\n Tadahiro Taniguchi\n\n taniguchi@i.kyoto-u.ac.jp \n\n Ryo Ueda\n\n Tomoaki Nakamura\n\n Masahiro Suzuki\n\n Akira Taniguchi\n\n###### Abstract\n\nLarge Language Models (LLMs) have demonstrated a remarkable ability to capture extensive world knowledge, yet how this is achieved without direct sensorimotor experience remains a fundamental puzzle. This study proposes a novel theoretical solution by introducing the Collective World Model hypothesis. We argue that an LLM does not learn a world model from scratch; instead, it learns a statistical approximation of a collective world model that is already implicitly encoded in human language through a society-wide process of embodied, interactive sense-making.\nTo formalize this process, we introduce generative emergent communication (Generative EmCom), a framework built on the Collective Predictive Coding (CPC). This framework models the emergence of language as a process of decentralized Bayesian inference over the internal states of multiple agents. We argue that this process effectively creates an encoder-decoder structure at a societal scale: human society collectively encodes its grounded, internal representations into language, and an LLM subsequently decodes these symbols to reconstruct a latent space that mirrors the structure of the original collective representations.\nThis perspective provides a principled, mathematical explanation for how LLMs acquire their capabilities. The main contributions of this paper are: 1) the formalization of the Generative EmCom framework, clarifying its connection to world models and multi-agent reinforcement learning, and 2) its application to interpret LLMs, explaining phenomena such as distributional semantics as a natural consequence of representation reconstruction. This work provides a unified theory that bridges individual cognitive development, collective language evolution, and the foundations of large-scale AI.\n\n###### keywords: \nemergent communication , large language model , world model , generative model , symbol emergence\n\n \\affiliation \n\n[1]organization=Graduate School of Informatics, Kyoto University,\ncountry=Japan\n \\affiliation [2]organization=Graduate School of Engineering, The University of Tokyo,\ncountry=Japan\n \\affiliation [3]organization=Graduate School of Informatics and Engineering, The University of Electro-Communications,\ncountry=Japan\n \\affiliation [4]organization=College of Information Science and Engineering, Ritsumeikan University,\ncountry=Japan\n\n 1 Introduction \n\nLarge Language Models (LLMs) have achieved astonishing success, exhibiting a profound capacity for reasoning and knowledge retrieval across countless domainsÂ  [ 12 , 83 ] . A central puzzle, however, lies in their apparent grasp of the structure of the physical world. LLMs, trained on vast corpora of text, are not designed to interact with an environment and lack any direct sensorimotor experienceÂ  [ 4 , 10 ] . Yet, they seem to possess what could be described as a â€œworld model.â€ How can an LLM acquire such a rich model of the world without ever having perceived or acted within it? This paper proposes a theoretical solution to this fundamental question.\n\nThe debate over whether LLMs truly possess world models is active and ongoing. A growing body of evidence suggests that LLMs acquire surprisingly rich representations of the real world. For instance, some studies show that LLMs implicitly learn representations corresponding to vision and hearing just by reading text [ 133 ] , and that specific spatiotemporal representations emerge within their activations [ 36 ] . Furthermore, this internal knowledge proves functionally potent; LLMs can be used as world models for complex planning tasks [ 46 ] , and their distributional semantics can be grounded in the physical world to guide robotic perception and action [ 93 , 142 ] . Conversely, other work has revealed that LLMs are not reliable world simulators, suggesting their internal models are brittle or fundamentally different from those of embodied agents [ 136 ] . This conflicting evidence highlights a deep conceptual gap in our understanding.\n\nTo clarify our argument, we distinguish between two concepts of a world modelÂ  [ 119 , 26 ] :\n\n (1) \n\n Type 1 World Model (Internal Model) : A subjective, internal model that an agent learns through its own sensorimotor interactions with the environment to predict future states and plan actions. This corresponds to the agentâ€™s Umwelt Â  [ 135 ] .\n\n (2) \n\n Type 2 World Model (Model of the World) : An objective, structured representation of knowledge about the world, its entities, and their relations, which may not be tied to a single agentâ€™s direct experience.\n\nWe can frame this distinction as one between a subjective Type 1 World Model and an objective Type 2 World Model . Much of the confusion in current discourse arises from this distinction; while the term â€world modelâ€ in AI often refers to a Type 1 model, particularly in the context of model-based reinforcement learning and predictive codingÂ  [ 38 , 40 , 39 , 35 , 119 ] , discussions about LLMs often imply they possess a Type 2 model. This presents a paradox: it is difficult to see how a disembodied LLM could form a subjective, embodied Type 1 model, yet it is equally unclear how it could acquire an objective Type 2 model without any access to the world.\n\nThis paper proposes a solution to this paradox. We argue that the seemingly objective knowledge within an LLM is, in fact, a pseudo-objective structure encoded in the distributional semantics of language. This structure emerges as a result of aggregating countless subjective, Type 1 world models from a society of embodied agents. From the perspective of generative emergent communication (Generative EmCom) an LLM does not model the world directly, but rather models the collective of these Type 1 models as externalized in language. This leads to our central hypothesis:\n\n {mdframed} \n\n[linewidth=0.5pt, roundcorner=5pt, innertopmargin=5pt, innerbottommargin=5pt]\n The Collective World Model Hypothesis \n\nHuman language is not merely a communication protocol but serves as an externalized representation of a collective world model , which emerges from the decentralized, interactive sense-making processes of an entire society of embodied agents. LLMs acquire their world knowledge by learning a statistical approximation of this collective world model encoded in text corpora.\n\nTo provide a formal basis for this hypothesis, we introduce a new theoretical framework called Generative EmCom . This framework is built upon the Collective Predictive Coding (CPC)Â  [ 117 ] , which extends the principles of predictive coding, the free-energy principle (FEP) and the Bayesian brain hypothesisÂ  [ 50 , 32 , 34 , 27 ] from individual brains to a societal level. Generative EmCom formalizes how a shared symbol system, i.e., language, emerges as multiple agents collectively seek to minimize their shared prediction errors about the world, a process we model as decentralized Bayesian inference.\n\nHowever, such an integrative theory has been largely missing. The series of studies on emergent communication (EmCom) and symbol emergence has attempted to explain the formation of languageÂ  [ 98 , 68 ] . Yet, these approaches have often failed to bridge the gap between two interdependent aspects: first, the learning of an individual agentâ€™s world model, which is grounded in its embodiment and environmental adaptationÂ  [ 35 , 119 ] ; and second, the collective emergence of a language that reflects this grounded knowledge. Our work aims to address this specific challenge by providing a unified framework.\n\nThis study, therefore, makes two primary contributions. First, we formalize the Generative EmCom framework, clarifying its relationship with conventional approaches and demonstrating its utility in multi-agent systems. Second, using this framework, we provide a principled, mathematical interpretation of LLMs as collective world models, offering a coherent explanation for their otherwise mysterious capabilities. This unified perspective bridges the gap between EmCom, cognitive development, and the foundations of large-scale AI, opening new frontiers for research. Indeed, the theoretical framework proposed herein has already begun to inspire new concrete models for multi-agent coordination in dynamic environmentsÂ  [ 88 ] and reward-independent communication in multi-agent reinforcement learning (MARL)Â  [ 141 ] .\n\nThe remainder of this paper is organized as follows: Section 2 reviews the current landscape of EmCom and world models, identifying the theoretical gap our work addresses. Section 3 details the theoretical framework of Generative EmCom. Section 4 explains how a collective world model can emerge from multi-agent interaction and cooperation. Section 5 applies this theory to reinterpret LLMs. Finally, Section 6 discusses the implications and limitations of our work, and Section 7 concludes the paper.\n\n 2 The Landscape of EmCom and World Models \n\nLanguage evolves and changes over time as a result of decentralized human communicationsÂ  [ 24 , 98 , 112 , 113 ] . Sentences are generated to describe a wide range of phenomena, including external events, emotions, and intentions. In particular, the system of language is inherently dyanamics rather than staticÂ  [ 137 , 114 , 126 , 123 ] . As Peirce, the founder of semiotics, suggested, symbols, including language, can be characterized by a triadic relationship of sign, object, and interpretantÂ  [ 95 , 96 , 18 ] . Here, sign corresponds to words, sentences and other signals. In particular, the correspondence between sign and object, which is signified by a sign (i.e., signifier), is determined by an interpretant. In other words, the meaning of a sign, that is, language, depends on culture and context, and so on.\n\n 2.1 Conventional Approaches to EmCom and Their Limits \n\nThe study of how communication protocols and symbolic language emerge from multi-agent interaction, a field known as EmCom, has been explored through several major approaches, including language games, MARL, and iterated learning models (ILMs)Â  [ 97 , 68 , 114 , 146 , 11 ] . A central theme in this research, particularly in studies employing language games and powered by deep neural networksÂ  [ 31 , 84 , 55 ] , is the extent to which the resulting protocols, or emergent languages (EmLangs), resemble human language. For instance, questions have been raised about whether EmLangs exhibit compositionalityÂ  [ 65 , 14 ] , follow well-known statistical properties of natural languagesÂ  [ 15 , 105 , 131 , 129 ] , or can be shaped by cognitive constraintsÂ  [ 104 , 57 ] .\n\nA foundational paradigm for many of these investigations is the Lewis signaling game Â  [ 73 ] . This approach, often aligned with Shannonâ€™s information theoryÂ  [ 110 ] , can be regarded as a discriminative model for optimizing a communication protocol.\n\nThe signaling game is a simple communication model that involves only a sender S Ï• â¢ ( m | x ) subscript ğ‘† bold-italic-Ï• conditional ğ‘š ğ‘¥ S_{\\boldsymbol{\\phi}}(m|x) italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) and a receiver R ğœ½ â¢ ( x | m ) subscript ğ‘… ğœ½ conditional ğ‘¥ ğ‘š R_{\\boldsymbol{\\theta}}(x|m) italic_R start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( italic_x | italic_m ) and only allows unidirectional communication from the sender to the receiver.\nAt each play, the signaling game proceeds as follows:\n\n 1. \n\n Observation : Sender S Ï• subscript ğ‘† bold-italic-Ï• S_{\\boldsymbol{\\phi}} italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT obtains an observation x ğ‘¥ x italic_x , that is, x âˆ¼ p â¢ ( x ) similar-to ğ‘¥ ğ‘ ğ‘¥ x\\sim p(x) italic_x âˆ¼ italic_p ( italic_x ) .\n\n 2. \n\n Signaling : Sender S Ï• subscript ğ‘† bold-italic-Ï• S_{\\boldsymbol{\\phi}} italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT generates a message m ğ‘š m italic_m from the observation x ğ‘¥ x italic_x , that is, m âˆ¼ S Ï• â¢ ( m | x ) similar-to ğ‘š subscript ğ‘† bold-italic-Ï• conditional ğ‘š ğ‘¥ m\\sim S_{\\boldsymbol{\\phi}}(m|x) italic_m âˆ¼ italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) .\n\n 3. \n\n Reconstruction : Receiver R ğœ½ subscript ğ‘… ğœ½ R_{\\boldsymbol{\\theta}} italic_R start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT attempts to reconstruct the original observation x ğ‘¥ x italic_x from the message m ğ‘š m italic_m via R ğœ½ â¢ ( x | m ) subscript ğ‘… ğœ½ conditional ğ‘¥ ğ‘š R_{\\boldsymbol{\\theta}}(x|m) italic_R start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( italic_x | italic_m ) .\n\nSender S Ï• subscript ğ‘† bold-italic-Ï• S_{\\boldsymbol{\\phi}} italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT and receiver R ğœ½ subscript ğ‘… ğœ½ R_{\\boldsymbol{\\theta}} italic_R start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT are optimized via a gradient-based method toward successful communication.\nConventionally, the objective function of the signaling game (to be maximized) is defined as follows [ 16 , 106 ] :\n\n ğ’¥ MI â¢ ( Ï• , ğœ½ ) := ğ”¼ p â¢ ( x ) , S Ï• â¢ ( m | x ) â¢ [ log â¡ R ğœ½ â¢ ( x | m ) ] . assign subscript ğ’¥ MI bold-italic-Ï• ğœ½ subscript ğ”¼ ğ‘ ğ‘¥ subscript ğ‘† bold-italic-Ï• conditional ğ‘š ğ‘¥ delimited-[] subscript ğ‘… ğœ½ conditional ğ‘¥ ğ‘š \\displaystyle\\mathcal{J}_{\\textup{MI}}(\\boldsymbol{\\phi},\\boldsymbol{\\theta})%\n\\mathrel{:=}\\mathbb{E}_{p(x),S_{\\boldsymbol{\\phi}}(m|x)}[\\log R_{\\boldsymbol{%\n\\theta}}(x|m)]. caligraphic_J start_POSTSUBSCRIPT MI end_POSTSUBSCRIPT ( bold_italic_Ï• , bold_italic_Î¸ ) := blackboard_E start_POSTSUBSCRIPT italic_p ( italic_x ) , italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) end_POSTSUBSCRIPT [ roman_log italic_R start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( italic_x | italic_m ) ] . \n\n (1) \n\nWe refer to ğ’¥ MI subscript ğ’¥ MI \\mathcal{J}_{\\textup{MI}} caligraphic_J start_POSTSUBSCRIPT MI end_POSTSUBSCRIPT as the mutual information (MI)-maximizing objective function because it is known to be an evidence lower bound (ELBO) (up to constant) of the following mutual information between X ğ‘‹ X italic_X and M ğ‘€ M italic_M [ 8 , 99 ] :\n\n I Ï• â¢ ( X ; M ) := ğ”¼ p â¢ ( x ) , S Ï• â¢ ( m | x ) â¢ [ log â¡ S Ï• â¢ ( m | x ) ğ”¼ p â¢ ( x â€² ) â¢ [ S Ï• â¢ ( m | x â€² ) ] ] . assign subscript ğ¼ bold-italic-Ï• ğ‘‹ ğ‘€ subscript ğ”¼ ğ‘ ğ‘¥ subscript ğ‘† bold-italic-Ï• conditional ğ‘š ğ‘¥ delimited-[] subscript ğ‘† bold-italic-Ï• conditional ğ‘š ğ‘¥ subscript ğ”¼ ğ‘ superscript ğ‘¥ â€² delimited-[] subscript ğ‘† bold-italic-Ï• conditional ğ‘š superscript ğ‘¥ â€² \\displaystyle I_{\\boldsymbol{\\phi}}(X;M)\\mathrel{:=}\\mathbb{E}_{p(x),S_{%\n\\boldsymbol{\\phi}}(m|x)}\\left[\\log\\frac{S_{\\boldsymbol{\\phi}}(m|x)}{\\mathbb{E}%\n_{p(x^{\\prime})}[S_{\\boldsymbol{\\phi}}(m|x^{\\prime})]}\\right]. italic_I start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_X ; italic_M ) := blackboard_E start_POSTSUBSCRIPT italic_p ( italic_x ) , italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) end_ARG start_ARG blackboard_E start_POSTSUBSCRIPT italic_p ( italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) end_POSTSUBSCRIPT [ italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT ) ] end_ARG ] . \n\n (2) \n\nThis implies that the conventional signaling game in the field of EmCom has been formulated as a problem of maximizing the mutual information between X ğ‘‹ X italic_X and M ğ‘€ M italic_M , where X ğ‘‹ X italic_X and M ğ‘€ M italic_M denote random variables corresponding to the realizations x ğ‘¥ x italic_x and m ğ‘š m italic_m , respectively.\n\nHowever, from the viewpoint of our paper, it is more fruitful to reinterpret these communication games through the lens of probabilistic generative models. This shift from a discriminative to a generative perspective reveals deeper connections to principles of cognition, learning, and the information-theoretic foundations of frameworks like the Variational Autoencoder (VAE) and the Information Bottleneck (IB) principleÂ  [ 3 , 144 ] .\n\nThis generative viewpoint reframes the signaling game as a problem of maximizing the ELBOÂ  [ 130 ] . The objective function becomes:\n\n ğ’¥ ELBO â¢ ( Ï• , ğœ½ ) := ğ”¼ p â¢ ( x ) [ ğ”¼ S Ï• â¢ ( m | x ) [ log R ğœ½ ( m | x ) ] âˆ’ Î² KL ( S Ï• ( m | x ) | | p ğœ½ ( m ) ) ] . \\displaystyle\\begin{split}\\mathcal{J}_{\\textup{ELBO}}(\\boldsymbol{\\phi},%\n\\boldsymbol{\\theta})&amp;\\mathrel{:=}\\mathbb{E}_{p(x)}[\\mathbb{E}_{S_{\\boldsymbol{%\n\\phi}}(m|x)}[\\log R_{\\boldsymbol{\\theta}}(m|x)]\\\\\n&amp;\\hphantom{\\mathrel{:=}\\mathbb{E}_{p(x)}[}-\\beta\\textrm{KL}(S_{\\boldsymbol{%\n\\phi}}(m|x)||p_{\\boldsymbol{\\theta}}(m))].\\end{split} start_ROW start_CELL caligraphic_J start_POSTSUBSCRIPT ELBO end_POSTSUBSCRIPT ( bold_italic_Ï• , bold_italic_Î¸ ) end_CELL start_CELL := blackboard_E start_POSTSUBSCRIPT italic_p ( italic_x ) end_POSTSUBSCRIPT [ blackboard_E start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) end_POSTSUBSCRIPT [ roman_log italic_R start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( italic_m | italic_x ) ] end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL - italic_Î² KL ( italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) | | italic_p start_POSTSUBSCRIPT bold_italic_Î¸ end_POSTSUBSCRIPT ( italic_m ) ) ] . end_CELL end_ROW \n\n (3) \n\nWe refer to ğ’¥ ELBO subscript ğ’¥ ELBO \\mathcal{J}_{\\textup{ELBO}} caligraphic_J start_POSTSUBSCRIPT ELBO end_POSTSUBSCRIPT as the ELBO-maximizing objective function , contrasting it with the MI-maximizing objective ğ’¥ MI subscript ğ’¥ MI \\mathcal{J}_{\\textup{MI}} caligraphic_J start_POSTSUBSCRIPT MI end_POSTSUBSCRIPT .\nBy adopting the ELBO-maximizing objective function, we can introduce concepts from computational psycholinguistics into signaling games.\nTo observe this, let us transform the ELBO maximizing objective function as follows:\n\n ğ’¥ ELBO â¢ ( Ï• , ğœ½ ) := ğ”¼ p â¢ ( x ) , S Ï• â¢ ( m | x ) â¢ [ log â¡ R Î¸ â¢ ( x | m ) âŸ communication + Î² â¢ log â¡ p Î¸ â¢ ( m ) âŸ (negative) surprisal ] âˆ’ Î² â¢ ğ”¼ p â¢ ( x ) â¢ â„‹ â¢ ( S Ï• â¢ ( M | x ) ) âŸ entropy maximizer , assign subscript ğ’¥ ELBO bold-italic-Ï• ğœ½ subscript ğ”¼ ğ‘ ğ‘¥ subscript ğ‘† bold-italic-Ï• conditional ğ‘š ğ‘¥ delimited-[] \n\n subscript âŸ subscript ğ‘… ğœƒ conditional ğ‘¥ ğ‘š communication ğ›½ subscript âŸ subscript ğ‘ ğœƒ ğ‘š (negative) surprisal ğ›½ subscript âŸ subscript ğ”¼ ğ‘ ğ‘¥ â„‹ subscript ğ‘† italic-Ï• conditional ğ‘€ ğ‘¥ entropy maximizer \\displaystyle\\begin{split}\\mathcal{J}_{\\textup{ELBO}}(\\boldsymbol{\\phi},%\n\\boldsymbol{\\theta})&amp;\\mathrel{:=}\\mathbb{E}_{p(x),S_{\\boldsymbol{\\phi}}(m|x)}[%\n\\underbrace{\\log R_{\\theta}(x|m)}_{\\textup{communication}}+\\beta\\underbrace{%\n\\log p_{\\theta}(m)}_{\\begin{subarray}{c}\\textup{(negative)}\\\\\n\\textup{surprisal}\\end{subarray}}]\\\\\n&amp;\\phantom{\\mathrel{:=}}-\\beta\\underbrace{\\mathbb{E}_{p(x)}\\mathcal{H}(S_{\\phi}%\n(M|x))}_{\\begin{subarray}{c}\\textup{entropy}\\\\\n\\textup{maximizer}\\end{subarray}},\\end{split} start_ROW start_CELL caligraphic_J start_POSTSUBSCRIPT ELBO end_POSTSUBSCRIPT ( bold_italic_Ï• , bold_italic_Î¸ ) end_CELL start_CELL := blackboard_E start_POSTSUBSCRIPT italic_p ( italic_x ) , italic_S start_POSTSUBSCRIPT bold_italic_Ï• end_POSTSUBSCRIPT ( italic_m | italic_x ) end_POSTSUBSCRIPT [ underâŸ start_ARG roman_log italic_R start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_x | italic_m ) end_ARG start_POSTSUBSCRIPT communication end_POSTSUBSCRIPT + italic_Î² underâŸ start_ARG roman_log italic_p start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_m ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL (negative) end_CELL end_ROW start_ROW start_CELL surprisal end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ] end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL - italic_Î² underâŸ start_ARG blackboard_E start_POSTSUBSCRIPT italic_p ( italic_x ) end_POSTSUBSCRIPT caligraphic_H ( italic_S start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT ( italic_M | italic_x ) ) end_ARG start_POSTSUBSCRIPT start_ARG start_ROW start_CELL entropy end_CELL end_ROW start_ROW start_CELL maximizer end_CELL end_ROW end_ARG end_POSTSUBSCRIPT , end_CELL end_ROW \n\n (4) \n\nwhere Î² ğ›½ \\beta italic_Î² is a hyperparameter. Here, a term known as surprisal appears, which is a concept commonly used in computational psycholinguistics [ 45 , 72 , 66 ] .\nSurprisal is assumed to represent the cognitive load experienced by a listener/reader (or the receiver in the signaling game) when processing a sentence.\nTherefore, the ELBO-maximizing signaling game naturally models the trade-off between information transmission and surprisal.\n\nA similar discussion involves modeling the trade-off between information transmission and efficiency, where studies have used the (variational) information bottleneck (IB, VIB) framework to model communication [ 144 , 17 , 127 ] .\nIn fact, VIB is proven to be a generalization of (beta-)VAE [ 3 , 1 ] , resulting in similar models. In addition, a contemporary work [ 128 ] shows that a variant framework known as a referential game can also be reformulated with an ELBO-like objective, analogous in structure to that of a conditional VAE [ 62 , 111 ] .\n\nIn relation to discussions in the field of evolutionary linguistics, some studies have also incorporated ILM into the EmCom framework [ 103 ] , which is another important research theme.\nThe ILM is a framework that models generational changes, where supervised learning is repeatedly performed from parent agents to child agents.\nIn the context of VAE and VIB, however, little discussion exists on modeling generational changes, and this remains a future challenge when considering generative symbol emergence.\n\nThe formulation of the signaling game presented in this section, that is, generative model-based re-formulation of conventional EmCom, shares some fundamental connections with the generative frameworks discussed in later sections. All these formulations can be interpreted as representation learning with messages serving as latent variables in generative models when using ELBO-type (or VIB-type) objectives. However, these signaling game-based approaches have inherent limitations: they typically assume a simple two-agent setting with asymmetric sender-receiver roles, and their extension to populated, decentralized settings is not straightforward.\n\nBeyond these foundational models, the field of Semantic Communication (SC) has gained traction, aiming to transmit information based on meaning and effectiveness rather than bit-level fidelityÂ  [ 100 ] . This has spurred research into areas such as deep learning-based semantic encodersÂ  [ 138 ] , contextual reasoning for shared understandingÂ  [ 109 ] , and learning goal-oriented languages from interactionÂ  [ 29 ] . A comprehensive framework, Emergent Semantic Communication (ESC), seeks to integrate many of these threads by using causal reasoning in a neuro-symbolic architecture to create efficient emergent languagesÂ  [ 125 ] . While these approaches advance the engineering of goal-oriented communication, they do not address how a society-wide, general-purpose language emerges to reflect a collective world model, which is the central question this paper addresses.\n\n 2.2 World Models for Individual Cognitive Agents \n\nThe concept of a world model represents an internal model within an agent that captures the dynamics of environmental states, their responses to the actions of the agent, and their relationships with sensory inputsÂ  [ 37 , 35 , 119 , 26 ] . The concept of world model has its origins in the early days of AI and robotics studiesÂ  [ 86 ] . Initial studies on ML investigated techniques for agents to autonomously construct and adapt their world modelsÂ  [ 108 , 116 ] . Currently, the term generally refers to predictive frameworksÂ  [ 39 ] , predominantly implemented using deep neural network architectures.\n\nThe concept of world models is closely related to the idea of predictive coding (PC). PC is the idea that the brain constantly predicts sensory information and updates its internal models to minimize prediction errors.\nIn the context of cognitive robotics and AI, world models provide the structure for representing and reasoning about the environment, whereas PC offers a mechanism for learning and updating these models based on sensory experiences. The free energy principle and active inference further unify these concepts, suggesting that both perception and action can be considered as processes of minimizing prediction errors or free energy. A theoretical connection exists between themÂ  [ 119 ] .\n\nGenerally, a theory of world models can be based on partially observable Markov decision process (POMDP). In this framework, the state z t subscript ğ‘§ ğ‘¡ z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT at time t ğ‘¡ t italic_t is not directly observable by the agent. Instead, the agent receives an observation x t subscript ğ‘¥ ğ‘¡ x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , which is assumed to be generated from a latent state z t subscript ğ‘§ ğ‘¡ z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . The agentâ€™s actions a t subscript ğ‘ ğ‘¡ a_{t} italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT influence the transition of states according to a probability distribution p â¢ ( z t + 1 | z t , a t ) ğ‘ conditional subscript ğ‘§ \n\n ğ‘¡ 1 subscript ğ‘§ ğ‘¡ subscript ğ‘ ğ‘¡ p(z_{t+1}|z_{t},a_{t}) italic_p ( italic_z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . The observation model is given by p â¢ ( x t | z t ) ğ‘ conditional subscript ğ‘¥ ğ‘¡ subscript ğ‘§ ğ‘¡ p(x_{t}|z_{t}) italic_p ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . The goal of the agent is to learn these probability distributions and use them to make predictions and inferences about the environment. This can be formalized as:\n\n State transition: \n z t + 1 âˆ¼ p â¢ ( z t + 1 | z t , a t ) similar-to subscript ğ‘§ \n\n ğ‘¡ 1 ğ‘ conditional subscript ğ‘§ \n\n ğ‘¡ 1 subscript ğ‘§ ğ‘¡ subscript ğ‘ ğ‘¡ \\displaystyle\\quad z_{t+1}\\sim p(z_{t+1}|z_{t},a_{t}) italic_z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT âˆ¼ italic_p ( italic_z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \n\n (5) \n\n Observation: \n x t âˆ¼ p â¢ ( x t | z t ) similar-to subscript ğ‘¥ ğ‘¡ ğ‘ conditional subscript ğ‘¥ ğ‘¡ subscript ğ‘§ ğ‘¡ \\displaystyle\\quad x_{t}\\sim p(x_{t}|z_{t}) italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ¼ italic_p ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) \n\n (6) \n\n Inference: \n z t âˆ¼ q â¢ ( z t | x 1 : t , a 1 : t âˆ’ 1 ) similar-to subscript ğ‘§ ğ‘¡ ğ‘ conditional subscript ğ‘§ ğ‘¡ subscript ğ‘¥ : 1 ğ‘¡ subscript ğ‘ : 1 ğ‘¡ 1 \\displaystyle\\quad z_{t}\\sim q(z_{t}|x_{1:t},a_{1:t-1}) italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ¼ italic_q ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 1 : italic_t - 1 end_POSTSUBSCRIPT ) \n\n (7) \n\nwhere q â¢ ( z t | x 1 : t , a 1 : t âˆ’ 1 ) ğ‘ conditional subscript ğ‘§ ğ‘¡ subscript ğ‘¥ : 1 ğ‘¡ subscript ğ‘ : 1 ğ‘¡ 1 q(z_{t}|x_{1:t},a_{1:t-1}) italic_q ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT 1 : italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT 1 : italic_t - 1 end_POSTSUBSCRIPT ) represents the agentâ€™s belief about the current state given the history of observations and actions. Learning these models enables the agent to construct a comprehensive world model that can be used for planning and decision-making in complex, partially observable environments.\n\nThis general framework has been instantiated in numerous influential modelsÂ  [ 26 ] . The â€œWorld Modelsâ€ approach by Ha and Schmidhuber, for instance, demonstrated that an agent could learn a compressed latent dynamics model from pixel inputs and use it to train a controller entirely within its own â€œdreamâ€Â  [ 38 , 37 ] . This line of research was significantly advanced by the Dreamer series of models, which successfully applied latent dynamics models to challenging continuous control tasks and even discrete Atari benchmarks and MinecraftÂ  [ 40 , 39 , 41 , 42 ] . While many of these models rely on reconstructing observations, alternative approaches have also been explored. For example, some models focus on learning latent dynamics through contrastive learning without reconstructionÂ  [ 67 , 89 , 90 ] , or by incorporating specific physical priors into the latent space, such as in NewtonianVAEÂ  [ 54 , 91 ] . Furthermore, to handle information from multiple viewpoints, which is crucial in multi-agent settings, approaches like Multi-View Dreaming have been proposed to construct a world model from multi-view images using contrastive learningÂ  [ 63 ] .\n\nA prominent recent approach to learning such world models is the Joint-Embedding Predictive Architecture (JEPA), proposed by LeCun as a key component of a pathway towards autonomous machine intelligenceÂ  [ 70 ] . Unlike generative approaches that attempt to predict missing information in pixel space, JEPA operates by predicting the representations of missing information in an abstract feature spaceÂ  [ 7 ] . This non-generative, self-supervised method aims to capture the underlying dependencies of the data, encouraging the model to learn semantic and predictive features rather than superficial details. The JEPA framework has proven effective for learning world models from various modalities, including images (I-JEPA)Â  [ 7 ] and videos (V-JEPA)Â  [ 9 ] . This line of research has recently culminated in V-JEPA 2, which extends the framework to enable robotic planning from video, further solidifying JEPA as a state-of-the-art method for an individual agent to develop its internal, predictive model of the worldÂ  [ 6 ] .\n\nIn summary, all the approaches discussed in this subsection, from classic POMDP-based models to modern architectures like JEPA, are fundamentally concerned with how a single agent learns a Type 1 World Model through its own interactions.\n\n 2.3 The Unaddressed Gap: From Individual Experience to Collective Language \n\nThe preceding review of research in EmCom (Section 2.1) and individual world models (Section 2.2) highlights a fundamental theoretical gap. On one hand, studies in EmCom have explored how agents can develop shared communication protocols, but these approaches often treat language as an abstract code, detached from the rich, embodied experiences that ground meaning in the real world. On the other hand, computational models and general theories that could provide a comprehensive and integrative understanding of symbol and language emergenceÂ  [ 120 , 123 ] are still lacking.\n\nSpecifically, these theories must address the critical interdependency between two aspects: first, the world modeling by individual agents, which is grounded in their embodiment and environmental adaptationÂ  [ 35 , 119 ] ; and second, the collective emergence of a language whose structure reflects this grounded world knowledge, for instance through distributional semantics Â  [ 47 , 36 ] . In essence, a crucial question remains unaddressed: What is the theoretical and computational mechanism that bridges the subjective, internal world of the individual with the objective, external world of collective language? Without a clear answer, our understanding of large-scale phenomena, such as the emergence of world knowledge in LLMs, remains incomplete. The need for a theoretical framework that can explain the dynamic and semantic aspects of language emergence in embodied cognitive developmental systems still remainsÂ  [ 79 , 123 , 13 , 117 ] , and this paper aims to provide such a framework.\n\n 3 Generative EmCom: A Theoretical Framework \n\nThese approaches somehow failed to construct a general framework capturing symbol emergence from the viewpoint perspective of general principles of environmental adaptation, such as the FEP, PC, and world modeling.\nRecently, world models have garnered significant attention as representation-learning models that incorporate action outputs and temporal dynamics of agentâ€“ environment interactionsÂ  [ 38 , 35 , 119 ] . This aligns with broader theoretical frameworks such as PC and the FEP. PC posits that the brain constantly predicts sensory information and updates its internal models to enhance predictability Â  [ 50 ] , whereas FEP provides a more generalized framework explaining the self-organization of biological systems through minimization of free energyÂ  [ 94 ] , which is associated with the idea of the Bayesian brainÂ  [ 27 ] . Notably, FEP extends beyond individual cognition to explain the self-organization of cognitive and biological systems in detailÂ  [ 64 , 33 , 20 ] , making it a promising foundation for understanding symbol emergence at both individual and collective levels.\n\n Figure 1 : \n(Left) probabilistic graphical model (PGM) representation of CPC in symbol emergence, that is, EmCom or language. The top-down generative process corresponds to language understanding and prediction of observations, which is downward causation in the symbol emergence system. The bottom-up inference process corresponds to perception, representation learning, and communication, that is, language game, which is upward causation in the symbol emergence system. (Right) Overview of the CPC in a symbol emergence system illustrating the bidirectional process of language understanding and generation, mediated by inference through language games and representation learning. \n\n 3.1 The CPC Hypothesis \n\nTo address the challenges outlined in Section 2, this paper builds upon the CPC hypothesisÂ  [ 117 , 121 ] . The idea extends the principles of PC and the FEP from the individual cognitive level to a societal levelÂ  [ 122 ] . CPC posits that the emergence of symbol systems, particularly language, can be modeled as a decentralized Bayesian inference of a shared latent representation. It assumes that not only individual agents but also entire groups engaged in symbolic communication can be modeled as generative systems, aiming to minimize their collective free energy (CFE).\n\nAlthough PC theory suggests that individual brains constantly predict sensory information and update their internal representations, including world models. CPC suggests that a group of agents, for instance, a human society, predict sensory information of all of the agents and update its external representations, that is, symbol systems.\n\nA question is raised. How can we update the external representations, e.g., language, while our brains are disconnected physically? The CPC hypothesis suggests that a type of language game performs a decentralized Bayesian inference among the group (e.g., Taniguchi etÂ al. [ 124 ], Hagiwara etÂ al. [ 44 ] ).\nIn this framework, language games (such as naming games) can be interpreted as implementing decentralized Bayesian inference of shared representations. A representative example is Metropolis-Hastings Naming Game (MHNG) explained in SectionÂ  3.3 .\nThe CPC hypothesis argues that symbol systems emerge as a result of decentralized Bayesian inference performed collaboratively by multiple agents.\n\nAlthough the encoding of sensory information through internal representations is ensured by the plasticity of neural systems, the plasticity of external representations is guaranteed by the flexibility of our symbol systems. The arbitrariness of symbol systems is a widely recognized characteristic of symbols in semioticsÂ  [ 18 ] . Peirce referred to the process by which subjects assign meaning to symbols according to culture and context as the semiosis .\nAlthough our brains are physically and electrically separated, they are informationally connected through communication using a flexible symbol system. Therefore, with appropriate communication and symbol system update algorithms, we can encode information into the symbol system as an external representation. In fact, the CPC hypothesis can consider that humans collectively perform this action in language emergence.\n\nThis implies that language collectively encodes information about the world as observed by numerous agents through their sensory-motor systems. The CPC hypothesis studyÂ  [ 117 ] did not provide a clear and detailed explanation regarding this point while proposing a new perspective on why LLMs seem to possess knowledge about the real world.\nThis is one of the main topics of this paper.\n\nEssentially, CPC hypothesizes that human language is formed through a process of collective PC, where the symbol system emerges to maximize the predictability of multi-modal sensory-motor information obtained by members of a society, that is, minimize the CFE of a group of agents. This approach provides a unified framework for understanding symbol emergence, language evolution, and the nature of linguistic knowledge from the perspective of environmental adaptation and brain science.\n\n 3.2 Formalizing Generative EmCom as Decentralized Bayesian Inference \n\nThe CPC hypothesis posits that a group of agents, interacting with each other and their environment, can be modeled as a single, large-scale probabilistic generative model. This allows us to formalize the emergence of a shared symbol system, such as language, as a process of decentralized Bayesian inference. For a group of K ğ¾ K italic_K agents, the generative and inference processes are defined as follows:\n\n Generative Model: \n p â¢ ( m , { z k } k , { x k } k | { a k } k ) ğ‘ ğ‘š subscript subscript ğ‘§ ğ‘˜ ğ‘˜ conditional subscript subscript ğ‘¥ ğ‘˜ ğ‘˜ subscript subscript ğ‘ ğ‘˜ ğ‘˜ \\displaystyle\\quad p(m,\\{z_{k}\\}_{k},\\{x_{k}\\}_{k}|\\{a_{k}\\}_{k}) italic_p ( italic_m , { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , { italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | { italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \n\n = p â¢ ( m ) â¢ âˆ k = 1 K p â¢ ( x k | z k , a k ) â¢ p â¢ ( z k | m , a k ) absent ğ‘ ğ‘š superscript subscript product ğ‘˜ 1 ğ¾ ğ‘ conditional subscript ğ‘¥ ğ‘˜ subscript ğ‘§ ğ‘˜ subscript ğ‘ ğ‘˜ ğ‘ conditional subscript ğ‘§ ğ‘˜ ğ‘š subscript ğ‘ ğ‘˜ \\displaystyle=p(m)\\prod_{k=1}^{K}p(x_{k}|z_{k},a_{k})p(z_{k}|m,a_{k}) = italic_p ( italic_m ) âˆ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) italic_p ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_m , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \n\n (8) \n\n Inference Model: \n q â¢ ( m , { z k } k | { x k } k , { a k } k ) ğ‘ ğ‘š conditional subscript subscript ğ‘§ ğ‘˜ ğ‘˜ subscript subscript ğ‘¥ ğ‘˜ ğ‘˜ subscript subscript ğ‘ ğ‘˜ ğ‘˜ \\displaystyle\\quad q(m,\\{z_{k}\\}_{k}|\\{x_{k}\\}_{k},\\{a_{k}\\}_{k}) italic_q ( italic_m , { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | { italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , { italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \n\n = q â¢ ( m | { z k } k ) â¢ âˆ k = 1 K q â¢ ( z k | x k , a k ) absent ğ‘ conditional ğ‘š subscript subscript ğ‘§ ğ‘˜ ğ‘˜ superscript subscript product ğ‘˜ 1 ğ¾ ğ‘ conditional subscript ğ‘§ ğ‘˜ subscript ğ‘¥ ğ‘˜ subscript ğ‘ ğ‘˜ \\displaystyle=q(m|\\{z_{k}\\}_{k})\\prod_{k=1}^{K}q(z_{k}|x_{k},a_{k}) = italic_q ( italic_m | { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_q ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \n\n (9) \n\nwhere for each agent k ğ‘˜ k italic_k , x k subscript ğ‘¥ ğ‘˜ x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is its observation, a k subscript ğ‘ ğ‘˜ a_{k} italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is its action, and z k subscript ğ‘§ ğ‘˜ z_{k} italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is its internal representation. The variable m ğ‘š m italic_m represents the shared, external symbol system (i.e., message) that connects the agents. The inference process q â¢ ( z k | x k , a k ) ğ‘ conditional subscript ğ‘§ ğ‘˜ subscript ğ‘¥ ğ‘˜ subscript ğ‘ ğ‘˜ q(z_{k}|x_{k},a_{k}) italic_q ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) corresponds to each agentâ€™s individual representation learning, while q â¢ ( m | { z k } k ) ğ‘ conditional ğ‘š subscript subscript ğ‘§ ğ‘˜ ğ‘˜ q(m|\\{z_{k}\\}_{k}) italic_q ( italic_m | { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) corresponds to the collective process of symbol emergence, which can be instantiated by a language game as discussed in Section 3.3.\n\nUnder the free energy principle, the goal of this collective system is to minimize the variational free energy (VFE), which is equivalent to maximizing the ELBO of the log-likelihood of observations. We term this the CFEÂ  [ 122 , 118 ] :\n\n F ğ¹ \\displaystyle F italic_F \n = D KL [ q ( m , { z k } k | { x k } k , { a k } k ) âˆ¥ p ( m , { z k } k | { x k } k , { a k } k ) ] \\displaystyle=D_{\\mathrm{KL}}\\left[q(m,\\{z_{k}\\}_{k}|\\{x_{k}\\}_{k},\\{a_{k}\\}_{%\nk})\\parallel p(m,\\{z_{k}\\}_{k}|\\{x_{k}\\}_{k},\\{a_{k}\\}_{k})\\right] = italic_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT [ italic_q ( italic_m , { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | { italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , { italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆ¥ italic_p ( italic_m , { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | { italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , { italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] \n\n = D KL â¢ [ q â¢ ( m | { z k } k ) âˆ¥ p â¢ ( m ) ] âŸ Collective Regularization absent subscript âŸ subscript ğ· KL delimited-[] conditional ğ‘ conditional ğ‘š subscript subscript ğ‘§ ğ‘˜ ğ‘˜ ğ‘ ğ‘š Collective Regularization \\displaystyle=\\underbrace{D_{\\mathrm{KL}}\\left[q(m|\\{z_{k}\\}_{k})\\|p(m)\\right]%\n}_{\\text{Collective Regularization}} = underâŸ start_ARG italic_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT [ italic_q ( italic_m | { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆ¥ italic_p ( italic_m ) ] end_ARG start_POSTSUBSCRIPT Collective Regularization end_POSTSUBSCRIPT \n\n + âˆ‘ k = 1 K ( ğ”¼ q â¢ [ âˆ’ log â¡ p â¢ ( x k | z k , a k ) ] âŸ Individual Prediction Error + D KL [ q ( z k | x k , a k ) âˆ¥ p ( z k | m , a k ) ] âŸ Individual Regularization ) \\displaystyle\\ \\ \\ +\\sum_{k=1}^{K}\\left(\\underbrace{\\mathbb{E}_{q}\\left[-\\log{%\np(x_{k}|z_{k},a_{k})}\\right]}_{\\text{Individual Prediction Error}}+\\underbrace%\n{D_{\\mathrm{KL}}\\left[q(z_{k}|x_{k},a_{k})\\|p(z_{k}|m,a_{k})\\right]}_{\\text{%\nIndividual Regularization}}\\right) + âˆ‘ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ( underâŸ start_ARG blackboard_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT [ - roman_log italic_p ( italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] end_ARG start_POSTSUBSCRIPT Individual Prediction Error end_POSTSUBSCRIPT + underâŸ start_ARG italic_D start_POSTSUBSCRIPT roman_KL end_POSTSUBSCRIPT [ italic_q ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) âˆ¥ italic_p ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT | italic_m , italic_a start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] end_ARG start_POSTSUBSCRIPT Individual Regularization end_POSTSUBSCRIPT ) \n\n (10) \n\nThis decomposition in Eq.Â ( 10 ) is central to our framework. It demonstrates that minimizing a single, unified objective function, the CFE, naturally gives rise to two distinct processes. The second term corresponds to each agent minimizing its own individual free energy, which involves learning to accurately reconstruct its observations (the prediction error term) while keeping its internal representations regularized (the individual regularization term). The first term, the Collective Regularization , drives the system to form a shared symbol m ğ‘š m italic_m . This term quantifies the cost of encoding the collective internal states { z k } k subscript subscript ğ‘§ ğ‘˜ ğ‘˜ \\{z_{k}\\}_{k} { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT into the shared message m ğ‘š m italic_m . The emergence of language is thus framed as a process that optimizes this trade-off between individual predictive accuracy and collective communicative efficiency.\n\nThis CFE formulation allows for a clear comparison with the ELBO-maximizing objective of signaling games presented in Section 2.1. While both are founded on the ELBO principle, they differ significantly in scope and structure. The signaling game ELBO typically models a two-agent, one-shot, and often asymmetric (sender/receiver) interaction. In contrast, the CFE provides a formulation for a population of K ğ¾ K italic_K agents engaged in a continuous, decentralized process of collective sense-making. The CFEâ€™s explicit decomposition into individual and collective terms provides a richer, more structured model of the interplay between individual learning and social language emergence, a distinction that conventional signaling game formulations do not typically make explicit.\n\n Table 1 : Nomenclature and parameter details \n\n m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \n\n Message (shared latent variable) communicated between agents at time t ğ‘¡ t italic_t \n\n o t k subscript superscript ğ‘œ ğ‘˜ ğ‘¡ o^{k}_{t} italic_o start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \n\n Optimality variable for the k ğ‘˜ k italic_k -th agent (1: optimal, 0: not optimal) \n\n z t k subscript superscript ğ‘§ ğ‘˜ ğ‘¡ z^{k}_{t} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \n\n Internal representation (e.g., state in RL) of the k ğ‘˜ k italic_k -th agent at time t ğ‘¡ t italic_t \n\n a t k subscript superscript ğ‘ ğ‘˜ ğ‘¡ a^{k}_{t} italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \n\n Action of the k ğ‘˜ k italic_k -th agent at time step t ğ‘¡ t italic_t \n\n x t k subscript superscript ğ‘¥ ğ‘˜ ğ‘¡ x^{k}_{t} italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \n\n Observations (or sensory inputs) of the k ğ‘˜ k italic_k -th agent at time t ğ‘¡ t italic_t \n\n Î¸ k superscript ğœƒ ğ‘˜ \\theta^{k} italic_Î¸ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT \n\n Global parameters of the internal models of the k ğ‘˜ k italic_k -th agent, \n for instance, neural networks \n\n Ï• k superscript italic-Ï• ğ‘˜ \\phi^{k} italic_Ï• start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT \n\n Parameters of language model of the k ğ‘˜ k italic_k -th agent. \n\n r k superscript ğ‘Ÿ ğ‘˜ r^{k} italic_r start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT \n\n Reward function for the k ğ‘˜ k italic_k -th agent \n\n p â¢ ( â‹… ) ğ‘ â‹… p(\\cdot) italic_p ( â‹… ) \n Original probability density function (i.e., a generative model) \n\n q â¢ ( â‹… ) ğ‘ â‹… q(\\cdot) italic_q ( â‹… ) \n Approximate probability function (i.e., an inference model) \n\n 3.3 The Naming Game as a Microcosm of Collective Inference \n\nThe hypothetical argument that language game can perform the decentralized Bayesian inference has a computational basis though whether actual language communication can realize such decentralized Bayesian inference in our human society is an open question. The MHNG is an instance of this idea.\n\nThe MHNG comprises the following steps:\n\n 1. \n\n Perception : Speaker and listener agents ( Sp and Li ) observe the d ğ‘‘ d italic_d -th object, obtain x d S â¢ p subscript superscript ğ‘¥ ğ‘† ğ‘ ğ‘‘ x^{Sp}_{d} italic_x start_POSTSUPERSCRIPT italic_S italic_p end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , and x d L â¢ i subscript superscript ğ‘¥ ğ¿ ğ‘– ğ‘‘ x^{Li}_{d} italic_x start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , and infer their internal representations z d S â¢ p superscript subscript ğ‘§ ğ‘‘ ğ‘† ğ‘ z_{d}^{Sp} italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S italic_p end_POSTSUPERSCRIPT and z d L â¢ i superscript subscript ğ‘§ ğ‘‘ ğ¿ ğ‘– z_{d}^{Li} italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT , respectively.\n\n 2. \n\n MH communication : Speaker mentions the name m d S â¢ p superscript subscript ğ‘š ğ‘‘ ğ‘† ğ‘ m_{d}^{Sp} italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S italic_p end_POSTSUPERSCRIPT of the d ğ‘‘ d italic_d -th object by sampling it from p â¢ ( m d | z d S â¢ p , Ï• S â¢ p ) ğ‘ conditional subscript ğ‘š ğ‘‘ superscript subscript ğ‘§ ğ‘‘ ğ‘† ğ‘ superscript italic-Ï• ğ‘† ğ‘ p(m_{d}|z_{d}^{Sp},\\phi^{Sp}) italic_p ( italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S italic_p end_POSTSUPERSCRIPT , italic_Ï• start_POSTSUPERSCRIPT italic_S italic_p end_POSTSUPERSCRIPT ) . The listener determines whether it accepts the naming with probability Î³ = min â¢ ( 1 , p â¢ ( z d L â¢ i | Ï• L â¢ i , m d S â¢ p ) p â¢ ( z d L â¢ i | Ï• L â¢ i , m d L â¢ i ) ) ğ›¾ min 1 ğ‘ conditional superscript subscript ğ‘§ ğ‘‘ ğ¿ ğ‘– superscript italic-Ï• ğ¿ ğ‘– superscript subscript ğ‘š ğ‘‘ ğ‘† ğ‘ ğ‘ conditional superscript subscript ğ‘§ ğ‘‘ ğ¿ ğ‘– superscript italic-Ï• ğ¿ ğ‘– superscript subscript ğ‘š ğ‘‘ ğ¿ ğ‘– \\gamma={\\rm min}\\left(1,\\frac{p(z_{d}^{Li}|{\\phi}^{Li},m_{d}^{Sp})}{p(z_{d}^{%\nLi}|{\\phi}^{Li},m_{d}^{Li})}\\right) italic_Î³ = roman_min ( 1 , divide start_ARG italic_p ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT | italic_Ï• start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT , italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S italic_p end_POSTSUPERSCRIPT ) end_ARG start_ARG italic_p ( italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT | italic_Ï• start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT , italic_m start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT ) end_ARG ) .\n\n 3. \n\n Learning : After MH communication was performed for every object, the listener updates its global parameters Î¸ L â¢ i superscript ğœƒ ğ¿ ğ‘– \\theta^{Li} italic_Î¸ start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT and Ï• L â¢ i superscript italic-Ï• ğ¿ ğ‘– \\phi^{Li} italic_Ï• start_POSTSUPERSCRIPT italic_L italic_i end_POSTSUPERSCRIPT .\n\n 4. \n\n Turn-taking : The speaker and listener alternate their roles and go back to (1).\n\nIt has been demonstrated that the MHNG is equivalent to the Metropolis-Hastings algorithm for inferring latent variables in a probabilistic generative model.\nAs a result, the MHNG protocol guarantees that the CFE, defined by the KL-divergence between the agentsâ€™ collective belief and the true posterior, is monotonically non-increasing in expectation. This is because the underlying distributed Metropolis-Hastings sampler satisfies the detailed balance condition, a well-established result related to the Data Processing InequalityÂ  [ 21 ] , ensuring convergence towards the target posterior distribution.\nThis model conditions the internal representations z k superscript ğ‘§ ğ‘˜ z^{k} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT of multiple agents, acting as representation learning machines, on a common external representation m ğ‘š m italic_m .\nAlthough the original study assumed two agents and a categorical message m ğ‘š m italic_m , the core probabilistic graphical model (PGM) of generative EmCom underlying this theory does not make these assumptions. Consequently, this fundamental idea can be extended in various ways.\n\nAs described, unlike referential games, MHNG assumes a joint attention performed by two agents 1 1 1 The assumption of joint attention is specific to the naming game, as its primary task is to establish a shared vocabulary for a common referent. However, the broader CPC framework does not depend on this. On the contrary, when applied to general multi-agent cooperation, its core strength lies in integrating different, partially observable perspectives, as we will discuss in Section 4. . This assumption may seem strange from the game-theoretic approach to EmCom, but it is a deliberate choice made to depart from the feedback-dependent principle of success or failure that underpins many such models. Developmental studies suggest that such explicit feedback interactions are rarely observed in natural language acquisition and are an implausible metaphor for how infants learnÂ  [ 126 ] . For human infants, joint attention is not a consequence of a communicative act, but rather a cognitive foundation that is in place before the vocabulary explosion occurs. Furthermore, children engaging in learning presuppose a pedagogical intention from adultsÂ  [ 23 ] , rather than simply testing a communication channel. Therefore, by assuming cooperative behavior grounded in joint attention, MHNG, as an example of generative EmCom, better aligns with empirical findings in human communication behavior than conventional discriminative models.\n\n Figure 2 : (Top) Overview of the MHNG process. In the game, which two agents (A and B) engage in, the agents perceive a target object with joint attention and form internal representations. One agent (speaker) utters a sign and the other (listener) determines whether to accept it. Thereafter, they take turns. (Bottom) PGM representation, which is assumed in the MHNG. The MHNG is proved to be an inference process in the representation learning in a collective multi-agent systemÂ  [ 124 ] . \n\nThe original idea of MH-based EmCom was introduced by Hagiwara etÂ al. [ 44 ] and later generalized and formalized by Taniguchi etÂ al. [ 124 ] , who clarified its connection to representation learning. The concept of MHNG has since been extended and validated in various ways. The extension to multimodal sensory information was achieved by Hagiwara etÂ al. [ 43 ] , who demonstrated that MHNG can lead to symbol emergence even when agents have different sensory modalities. Their work also showed that modality information possessed by other agents, but not by oneself, can facilitate object category formation.\nSimilar multimodal extensions have been explored with variational autoencoder (VAE)-based representation learning by Hoang etÂ al. [ 48 ] , though their work suggests that the mechanism for integrating multimodal sensory information influences symbol emergence.\nAlthough these MHNG studies involve two agents, a mathematical extension to N ğ‘ N italic_N -agent conditions was developed by Inukai etÂ al. [ 52 ] , who introduced a recursive structure in communication while maintaining its characteristics as distributed Bayesian inference. They argued that random partner selection for MHNG can be considered as a one-sample and limited-length approximation of this approach.\n\nThe Generative EmCom framework is general, and its core mechanisms are not limited to the exchange of simple categorical signs; they can be readily extended to handle complex, compositional languages. A recent study by LeÂ Hoang etÂ al. [ 69 ] demonstrates that sharing compositional word sequences is possible within the MHNG framework, similar to numerous EmCom studies. Furthermore, Matsui etÂ al. [ 80 ] extended this concept to a full captioning game, where multiple Vision-Language Models interactively generate and refine natural language captions for images. From Peirceâ€™s semiotic perspective, signs include both compositional discrete sequences and continuous signs such as voice pitch and facial expressions. A recent study by You etÂ al. [ 143 ] demonstrated that such continuous signs can emerge within the generative EmCom framework, whereas Saito etÂ al. [ 107 ] modeled the emergence of compositional signs from continuous time-series information as signals.\n\nTo verify the CPC hypothesis, examining whether human sign acceptance rates in joint attention naming scenarios match MHNG predictions was necessary. An experimental semiotic study by Okumura etÂ al. [ 92 ] demonstrated that the sign acceptance rate in MHNG effectively predicts human behavior.\n\nMHNG serves as a basic language game to represent symbol emergence by realizing distributed Bayesian inference and inferring latent variables of the generative model, corresponding to language. The critical point is that the inference of the posterior distribution is performed in a decentralized manner through language games. This suggests that being based on the MH method is not a necessary condition. The MHNG-based approach represents an initial step in modeling symbol emergence (or language emergence, EmCom) as decentralized Bayesian inference. Future studies should explore the possibility of constructing generative EmCom models by distributing various inference methodsÂ (e.g., Hoang etÂ al. [ 49 ] ).\n\n 4 Emergence of a Collective World Model through Multi-Agent Interaction and Cooperation \n\nThe previous section used the MHNG to illustrate decentralized inference under the simplified assumption of joint attention. This section expands upon that principle, turning to more general multi-agent systems where joint attention is not required. Here, the essence of the CPC framework becomes clear: to integrate the diverse, partially observable perspectives of multiple agents through communication. We will explore how an emergent language allows agents to resolve their individual uncertainties and coordinate their actions by inferring a shared understanding of the world. This exploration begins with the application of Generative EmCom to MARL, considering that Type 1 world models are theoretically rooted in reinforcement learning studies.\n\n 4.1 From Communication to Cooperation in Multi-Agent Systems \n\n Figure 3 : Graphical model of generating cooperative actions for two agents. \n\nCommunication and language are often considered to emerge to facilitate multi-agent cooperation.\nIn recent years, studies on MARL with communication channels have been progressing.\nInitial methods in multi-agent deep RL include DIAL [ 30 ] and CommNet [ 115 ] . These methods connect the networks of agents through messages, enabling the learning of necessary messages for cooperative behavior through backpropagation. Additionally, multi-agent deep deterministic policy gradient (MADDPG), an extension of the DDPG [ 75 ] for MARL, has been proposed [ 78 ] .\nIn these methods, the agents use messages sent from all other agents to learn their policies.\nMethods involving weighting or attention mechanisms have been developed to limit communication to only necessary agents [ 59 , 56 , 60 , 53 ] . To compute efficient messages, graph neural networks (GNNs) are used [ 19 , 76 , 2 , 87 , 101 ] .\nIn these studies, multiple agents are connected through a network and messages are inferred by making them differentiable variables through backpropagation. In other words, error information computed from the internal states of others is directly transmitted to oneself, which is an unnatural modeling from the perspective of communication among independent individuals.\n\nIn contrast, generative EmCom allows us to formulate MARL with EmCom in a Bayesian manner by incorporating the idea of CaI [ 71 ] , which is a theory to formulate RL as a PGM [ 28 , 85 ] .\nFigure 3 shows a graphical model of cooperative action generation between two agents, and the details of each stochastic variable are listed in Table 1 .\nThe behavior of each agent is generated through a Markov decision process with a prior variable m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .\nThe state z t k subscript superscript ğ‘§ ğ‘˜ ğ‘¡ z^{k}_{t} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of an agent at time t ğ‘¡ t italic_t is determined according to state z t âˆ’ 1 k subscript superscript ğ‘§ ğ‘˜ ğ‘¡ 1 z^{k}_{t-1} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , action a t âˆ’ 1 k subscript superscript ğ‘ ğ‘˜ ğ‘¡ 1 a^{k}_{t-1} italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , and message m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , which is the shared latent variable:\n\n z t k âˆ¼ p â¢ ( z t k | m t , z t âˆ’ 1 k , a t âˆ’ 1 k ) . similar-to subscript superscript ğ‘§ ğ‘˜ ğ‘¡ ğ‘ conditional subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript ğ‘š ğ‘¡ subscript superscript ğ‘§ ğ‘˜ ğ‘¡ 1 subscript superscript ğ‘ ğ‘˜ ğ‘¡ 1 \\displaystyle z^{k}_{t}\\sim p(z^{k}_{t}|m_{t},z^{k}_{t-1},a^{k}_{t-1}). italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ¼ italic_p ( italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ) . \n\n (11) \n\nwhere k âˆˆ { A , B } ğ‘˜ ğ´ ğµ k\\in\\{A,B\\} italic_k âˆˆ { italic_A , italic_B } denotes an index of agents.\nThe agent can indirectly infer the state of others through the message m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT in a probabilistic manner.\n\nThe optimality variable o t k âˆˆ { 0 , 1 } subscript superscript ğ‘œ ğ‘˜ ğ‘¡ 0 1 o^{k}_{t}\\in\\{0,1\\} italic_o start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ { 0 , 1 } represents the state optimality of both agents: 1 1 1 1 indicates that the state and action pair are on the optimal trajectories, whereas 0 0 indicates it is not.\nNote that optimality is a type of probabilistic interpretation of reward functions as shown below.\nIn this model, two types of optimality exist: one for each individual agent and the other for the group of agents.\nThe probability p â¢ ( o t k = 1 | z t k , a t k ) ğ‘ subscript superscript ğ‘œ ğ‘˜ ğ‘¡ conditional 1 subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ p(o^{k}_{t}=1|z^{k}_{t},a^{k}_{t}) italic_p ( italic_o start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 | italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) of this optimality variable is computed using reward function r k â¢ ( z t k , a t k ) superscript ğ‘Ÿ ğ‘˜ subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ r^{k}(z^{k}_{t},a^{k}_{t}) italic_r start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) as follows:\n\n p â¢ ( o t k = 1 | z t k , a t k ) âˆ exp â¡ ( r â¢ ( z t k , a t k ) ) . proportional-to ğ‘ subscript superscript ğ‘œ ğ‘˜ ğ‘¡ conditional 1 subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ ğ‘Ÿ subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ \\displaystyle p(o^{k}_{t}=1|z^{k}_{t},a^{k}_{t})\\propto\\exp(r(z^{k}_{t},a^{k}_%\n{t})). italic_p ( italic_o start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 | italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) âˆ roman_exp ( italic_r ( italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) . \n\n (12) \n\nFor the group of agents, the true optimality o t A â¢ B subscript superscript ğ‘œ ğ´ ğµ ğ‘¡ o^{AB}_{t} italic_o start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT depends on the joint state and actions, with its probability being proportional to a global reward function r A â¢ B superscript ğ‘Ÿ ğ´ ğµ r^{AB} italic_r start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT :\n\n p â¢ ( o t A â¢ B = 1 | z t A , a t A , z t B , a t B ) âˆ exp â¡ ( r A â¢ B â¢ ( z t A , a t A , z t B , a t B ) ) . proportional-to ğ‘ subscript superscript ğ‘œ ğ´ ğµ ğ‘¡ conditional 1 subscript superscript ğ‘§ ğ´ ğ‘¡ subscript superscript ğ‘ ğ´ ğ‘¡ subscript superscript ğ‘§ ğµ ğ‘¡ subscript superscript ğ‘ ğµ ğ‘¡ superscript ğ‘Ÿ ğ´ ğµ subscript superscript ğ‘§ ğ´ ğ‘¡ subscript superscript ğ‘ ğ´ ğ‘¡ subscript superscript ğ‘§ ğµ ğ‘¡ subscript superscript ğ‘ ğµ ğ‘¡ \\displaystyle p(o^{AB}_{t}=1|z^{A}_{t},a^{A}_{t},z^{B}_{t},a^{B}_{t})\\propto%\n\\exp(r^{AB}(z^{A}_{t},a^{A}_{t},z^{B}_{t},a^{B}_{t})). italic_p ( italic_o start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 | italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) âˆ roman_exp ( italic_r start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) . \n\n (13) \n\nHowever, since each agent k ğ‘˜ k italic_k cannot observe othersâ€™ internal variables, it must learn an approximate model of this group optimality conditioned only on its own state z t k subscript superscript ğ‘§ ğ‘˜ ğ‘¡ z^{k}_{t} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and action a t k subscript superscript ğ‘ ğ‘˜ ğ‘¡ a^{k}_{t} italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Agent k ğ‘˜ k italic_k therefore learns its own predictive model of group success as:\n\n p â¢ ( o t A â¢ B = 1 | z t k , a t k ) âˆ exp â¡ ( r ^ k A â¢ B â¢ ( z t k , a t k ) ) , proportional-to ğ‘ subscript superscript ğ‘œ ğ´ ğµ ğ‘¡ conditional 1 subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ subscript superscript ^ ğ‘Ÿ ğ´ ğµ ğ‘˜ subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ \\displaystyle p(o^{AB}_{t}=1|z^{k}_{t},a^{k}_{t})\\propto\\exp(\\hat{r}^{AB}_{k}(%\nz^{k}_{t},a^{k}_{t})), italic_p ( italic_o start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 | italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) âˆ roman_exp ( over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) , \n\n (14) \n\nwhere r ^ k A â¢ B â¢ ( z t k , a t k ) subscript superscript ^ ğ‘Ÿ ğ´ ğµ ğ‘˜ subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ \\hat{r}^{AB}_{k}(z^{k}_{t},a^{k}_{t}) over^ start_ARG italic_r end_ARG start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is a function learned by agent k ğ‘˜ k italic_k that locally approximates the global reward. This formulation eliminates the necessity for an agent to access othersâ€™ internal variables to model group success, while still allowing it to contribute to the collective goal.\n\nFollowing the theory of CaI, the optimal state sequence for both agents can be calculated by inferring state z t subscript ğ‘§ ğ‘¡ z_{t} italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and message m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT under the condition that the value of the optimality variables is always 1, as if the two-agent system acts as a single agent:\n\n z t A , m t âˆ¼ p ( z t A , m t | z t B , o 1 : T A = ğŸ , o 1 : T A â¢ B = ğŸ ) . \\displaystyle z^{A}_{t},m_{t}\\sim p(z^{A}_{t},m_{t}|z^{B}_{t},o^{A}_{1:T}={%\n\\mathbf{1}},o^{AB}_{1:T}={\\mathbf{1}}). italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆ¼ italic_p ( italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_o start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT = bold_1 , italic_o start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT = bold_1 ) . \n\n (15) \n\nHowever, this equation has two problems: it includes the internal state z t B subscript superscript ğ‘§ ğµ ğ‘¡ z^{B}_{t} italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of others, which cannot be observed in practice, and deriving this probability distribution analytically is difficult.\nWe solve these problems by alternately inferring the following two variables:\n\n z 1 : T A , a 1 : T A âˆ¼ p ( z 1 : T A , a 1 : T A | o 1 : T A = ğŸ , m 1 : T ) z 1 : T B , a 1 : T B âˆ¼ p ( z 1 : T B , a 1 : T B | o 1 : T B = ğŸ , m 1 : T ) } : planning , \\displaystyle\\left.\\begin{aligned} z^{A}_{1:T},a^{A}_{1:T}&amp;\\sim p(z^{A}_{1:T},%\na^{A}_{1:T}|o^{A}_{1:T}={\\mathbf{1}},m_{1:T})\\\\\nz^{B}_{1:T},a^{B}_{1:T}&amp;\\sim p(z^{B}_{1:T},a^{B}_{1:T}|o^{B}_{1:T}={\\mathbf{1}%\n},m_{1:T})\\end{aligned}~{}~{}\\right\\}{\\rm:planning,} start_ROW start_CELL italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT end_CELL start_CELL âˆ¼ italic_p ( italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT | italic_o start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT = bold_1 , italic_m start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT ) end_CELL end_ROW start_ROW start_CELL italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT end_CELL start_CELL âˆ¼ italic_p ( italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT | italic_o start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT = bold_1 , italic_m start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT ) end_CELL end_ROW } : roman_planning , \n\n (16) \n\n m 1 : T âˆ¼ p â¢ ( m 1 : T | z 1 : T A , z 1 : T B , o 1 : T A â¢ B = ğŸ ) : communication . : similar-to subscript ğ‘š : 1 ğ‘‡ ğ‘ conditional subscript ğ‘š : 1 ğ‘‡ subscript superscript ğ‘§ ğ´ : 1 ğ‘‡ subscript superscript ğ‘§ ğµ : 1 ğ‘‡ subscript superscript ğ‘œ ğ´ ğµ : 1 ğ‘‡ 1 communication \\displaystyle m_{1:T}\\sim p(m_{1:T}|z^{A}_{1:T},z^{B}_{1:T},o^{AB}_{1:T}={%\n\\mathbf{1}}){\\rm:communication.} italic_m start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT âˆ¼ italic_p ( italic_m start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT , italic_o start_POSTSUPERSCRIPT italic_A italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT = bold_1 ) : roman_communication . \n\n (17) \n\nEquation ( 16 ) describes state planning, which can be computed based on the CaI framework [ 71 ] .\nEquation ( 17 ) describes the inference of the message and can be formulated using the MHNG proposed by Hagiwara etÂ al. [ 43 ] and Taniguchi etÂ al. [ 124 ] , which allows both agents to infer messages through communication without observing each otherâ€™s internal states. Note that MHNG enables two agents to perform sampling in ( 17 ) without simultaneous observations of z t A , z t B subscript superscript ğ‘§ ğ´ ğ‘¡ subscript superscript ğ‘§ ğµ ğ‘¡ z^{A}_{t},z^{B}_{t} italic_z start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT .\n\nThus, MHNG can be used not only for multimodal object categorization and naming but also for action coordination among multi-agents using PGM to formulate MARL, that is, modeling EmCom for multi-agent cooperation.\nThis theoretical connection has been recently instantiated in the MARL-CPC framework, which successfully applies the principles of CPC to on-policy RL algorithms and demonstrates effective communication in non-cooperative settingsÂ  [ 141 ] .\n\n Figure 4 : Graphical model of generative EmCom involving world models. \n\n 4.2 Language as an External Representation for Integrating World Models \n\nAs established in Section 2.2, an individual agentâ€™s world model is an internal mechanism used to predict future latent states based on current states and actions, i.e., it learns the state transition dynamics p â¢ ( z t + 1 | z t , a t ) ğ‘ conditional subscript ğ‘§ \n\n ğ‘¡ 1 subscript ğ‘§ ğ‘¡ subscript ğ‘ ğ‘¡ p(z_{t+1}|z_{t},a_{t}) italic_p ( italic_z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . In a multi-agent setting where each agent has only partial observations of the environment, each agent learns its own subjective world model. Without a mechanism for exchanging information, these individual world models remain isolated, and the agents lack a way to form a shared, consistent understanding of the global environment.\n\nOur Generative EmCom framework introduces such a mechanism by incorporating a shared latent variable, the message m ğ‘š m italic_m , into the world model of each agent. This fundamentally alters the state transition dynamics. As depicted in the PGM for a single agent with communication (Figure 4 ), the world model of agent k ğ‘˜ k italic_k , parameterized by Î¸ k superscript ğœƒ ğ‘˜ \\theta^{k} italic_Î¸ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , now learns the dynamics conditioned on the message: p â¢ ( z t + 1 k | z t k , a t k , m t ) ğ‘ conditional subscript superscript ğ‘§ ğ‘˜ \n\n ğ‘¡ 1 subscript superscript ğ‘§ ğ‘˜ ğ‘¡ subscript superscript ğ‘ ğ‘˜ ğ‘¡ subscript ğ‘š ğ‘¡ p(z^{k}_{t+1}|z^{k}_{t},a^{k}_{t},m_{t}) italic_p ( italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT | italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . Here, the message m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is not part of the action space; instead, it acts as a conditioning variable that directly influences the agentâ€™s prediction of its future internal state z t + 1 k subscript superscript ğ‘§ ğ‘˜ \n\n ğ‘¡ 1 z^{k}_{t+1} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT . This latent state, in turn, informs the agentâ€™s policy, Ï€ k superscript ğœ‹ ğ‘˜ \\pi^{k} italic_Ï€ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , which generates the subsequent action.\n\nThis architecture creates a powerful feedback loop for knowledge integration. As formalized in our framework (Section 3.2), the message m t subscript ğ‘š ğ‘¡ m_{t} italic_m start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is inferred from the collective internal states of all agents, { z t k } k subscript subscript superscript ğ‘§ ğ‘˜ ğ‘¡ ğ‘˜ \\{z^{k}_{t}\\}_{k} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . In turn, this collectively-informed message influences the future internal state of each individual agent. Therefore, the process of learning to use the language m ğ‘š m italic_m effectively becomes a process of learning to align these individual world models. Each agent learns to encode its own unique knowledge into the shared language and to decode the language to update its own world model with information it could not observe directly. This perspective aligns with recent works that have demonstrated the effectiveness of message-conditioned world models for improving coordinationÂ  [ 22 , 77 , 88 ] .\n\nThrough this process, the emergent language m ğ‘š m italic_m becomes more than just a set of signals for coordinating immediate actions. It becomes the shared, external representation that embodies the integrated knowledge of the individual world models. It is the medium through which a collective of agents, each with a subjective Type 1 World Model, can construct a shared, objective Type 2 World Model (i.e., Model of the World), allowing the group to model the world from a unified perspective that transcends any single agentâ€™s limited view.\n\n Figure 5 : Three levels of the relationship between language, perceptual, and action information. Left: (A) Image captioning and generation tasks corresponding language to a still image. (B) Video captioning and generation tasks corresponding language to a video, that is, a sequence of visual stimuli. (C) Action-dependent video captioning and generation corresponding language to dynamic perceptual and action information, which corresponds to a world model and a policy conditioned by language. Right: probabilistic generative models corresponding to each of (A) â€“ (C). \n\n Figure 6 : PGMs for generative EmCom corresponding to three levels of complexity: (A) image-, (B) video-, and (C) action- and video-based tasks. These models represent instances of the CPC hypothesis and generalizations of MHNG studies. \n\n 4.3 Collective World Model as an Abstractive Prior over Embodied Experiences \n\nTo understand how language integrates individual world models into a collective one, we first consider the hierarchy of experiences that language can represent, as illustrated in FigureÂ  5 . This hierarchy progresses through three levels of increasing complexity. Level A represents the bidirectional relationship between a static observation, such as an image x k subscript ğ‘¥ ğ‘˜ x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , and a linguistic description m ğ‘š m italic_m . This includes both image captioning (inferring m ğ‘š m italic_m from x k subscript ğ‘¥ ğ‘˜ x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) and image generation (generating x k subscript ğ‘¥ ğ‘˜ x_{k} italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT from m ğ‘š m italic_m )Â  [ 134 , 139 , 102 ] . Level B extends this to dynamic sequences, where the language m ğ‘š m italic_m corresponds to a stream of observations over time, { x t k } t = 1 : T subscript subscript superscript ğ‘¥ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ \\{x^{k}_{t}\\}_{t=1:T} { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT , as seen in video captioning and generation tasksÂ  [ 132 , 140 ] . The hierarchy culminates in Level C, which models the full sensorimotor loop of an embodied agent. Here, language m ğ‘š m italic_m is grounded in both dynamic perception { x t k } t = 1 : T subscript subscript superscript ğ‘¥ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ \\{x^{k}_{t}\\}_{t=1:T} { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT and a sequence of actions { a t k } t = 1 : T subscript subscript superscript ğ‘ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ \\{a^{k}_{t}\\}_{t=1:T} { italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT . This final level corresponds to the world models used in modern robotics, often called Vision-Language-Action (VLA) models, which learn from an agentâ€™s own embodied interactionsÂ  [ 5 , 61 , 25 , 145 , 58 ] .\n\nCrucially, in a collective setting, the shared message m ğ‘š m italic_m is inferred from the observations of all K ğ¾ K italic_K agents, e.g., m âˆ¼ q â¢ ( m | { x 1 : T k } k ) similar-to ğ‘š ğ‘ conditional ğ‘š subscript subscript superscript ğ‘¥ ğ‘˜ : 1 ğ‘‡ ğ‘˜ m\\sim q(m|\\{x^{k}_{1:T}\\}_{k}) italic_m âˆ¼ italic_q ( italic_m | { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . This collective inference process itself constitutes a form of (external) representation learning, where the structure of the shared language m ğ‘š m italic_m comes to reflect the integrated knowledge of the entire group.\n\nThe generative process for each of these three levels can be explicitly formulated as a Probabilistic Graphical Model (PGM), as shown in FigureÂ  6 . At Level A, the PGM captures the relationship between a static image observation x k superscript ğ‘¥ ğ‘˜ x^{k} italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT and a message m ğ‘š m italic_m , mediated by an internal representation z k superscript ğ‘§ ğ‘˜ z^{k} italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT .\nAt Level B, this model is extended to handle temporal dynamics, where a sequence of video frames { x 1 : T k } subscript superscript ğ‘¥ ğ‘˜ : 1 ğ‘‡ \\{x^{k}_{1:T}\\} { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } informs a sequence of internal states { z 1 : T k } subscript superscript ğ‘§ ğ‘˜ : 1 ğ‘‡ \\{z^{k}_{1:T}\\} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } .\nAs shown in the PGMs for Level A and Level B (FigureÂ  6 Â (A) and (B)), the processes are governed by two sets of parameters for each agent k ğ‘˜ k italic_k : Î¸ k superscript ğœƒ ğ‘˜ \\theta^{k} italic_Î¸ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT represents the parameters of the agentâ€™s internal model (i.e., the VLM), while Ï• k superscript italic-Ï• ğ‘˜ \\phi^{k} italic_Ï• start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT denotes the parameters of the prior over the shared message m ğ‘š m italic_m .\nFinally, Level C depicts the complete generative model for an embodied agent, where both sensory observations { x 1 : T k } subscript superscript ğ‘¥ ğ‘˜ : 1 ğ‘‡ \\{x^{k}_{1:T}\\} { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } and actions { a 1 : T k } subscript superscript ğ‘ ğ‘˜ : 1 ğ‘‡ \\{a^{k}_{1:T}\\} { italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } jointly determine the agentâ€™s internal state trajectory. The parameters of this comprehensive model, Î¸ Â¯ k superscript Â¯ ğœƒ ğ‘˜ \\bar{\\theta}^{k} overÂ¯ start_ARG italic_Î¸ end_ARG start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , therefore encompass not only the agentâ€™s predictive world model but also its policy. These PGMs represent concrete instances of the overarching CPC hypothesis.\n\nThis principle generalizes directly as we move up the hierarchy to a collective of fully embodied VLA agents (Level C). Here, the framework models a population of agents, each possessing its own sophisticated world model for navigating its environment (FigureÂ  6 Â (C)). The interactive, decentralized process of these agents developing a shared language to communicate about their sensorimotor experiences ( { x 1 : T k } k , { a 1 : T k } k ) subscript subscript superscript ğ‘¥ ğ‘˜ : 1 ğ‘‡ ğ‘˜ subscript subscript superscript ğ‘ ğ‘˜ : 1 ğ‘‡ ğ‘˜ (\\{x^{k}_{1:T}\\}_{k},\\{a^{k}_{1:T}\\}_{k}) ( { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , { italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_T end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) becomes mathematically equivalent to the process of learning a single, abstract, hierarchical world model that spans the entire group. Therefore, we argue that embodied symbol emergence , under the CPC framework, is collective world modeling.\n\nThe emergent language m ğ‘š m italic_m is not merely a tool for this process; it is the very instantiation of the Collective World Model. It functions as a highly abstract structured prior over the entire space of possible embodied experiences within the society. The dynamic, interactive process of collective sense-making described above does not simply vanish; it leaves behind a static artifact in the form of a text corpus. We can think of this corpus as a â€œfossil recordâ€ of a societyâ€™s ongoing, collective inference about the world 2 2 2 This idea has also been applied to model the process of scientific activity itself from the perspective of CPC, a framework known as CPC as a model of science (CPC-MS)Â  [ 122 ] . .\nFrom this perspective, each sentence or document within a corpus can be viewed as a sample from an underlying posterior distribution. More formally, we consider a sentence m j [ i ] superscript subscript ğ‘š ğ‘— delimited-[] ğ‘– m_{j}^{[i]} italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT [ italic_i ] end_POSTSUPERSCRIPT describing a particular event or situation j ğ‘— j italic_j to be a sample from the approximate posterior q â¢ ( m | { { x t k } t = 1 : T , { a t k } t = 1 : T } k ) ğ‘ conditional ğ‘š subscript subscript subscript superscript ğ‘¥ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ subscript subscript superscript ğ‘ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ ğ‘˜ q(m|\\{\\{x^{k}_{t}\\}_{t=1:T},\\{a^{k}_{t}\\}_{t=1:T}\\}_{k}) italic_q ( italic_m | { { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT , { italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . It is this product, rich with the implicitly encoded structure of the collective world model, that serves as the training data for LLMs.\n\n 5 LLM as Collective World models \n\n 5.1 Language Corpora as Samples from the Collective World Model \n\nThe preceding sections have established our central premise: that human language, as a product of CPC, can be understood as a Collective World Model externalized in a shared symbolic system. Building on this foundation, we now connect this theory to the training of LLMs.\n\nAs argued in Section 4.3, each sentence m ğ‘š m italic_m within a text corpus is not an isolated artifact but can be viewed as a sample drawn from a complex posterior distribution, conditioned on the unobserved, collective sensorimotor experiences of the society that generated it:\n\n m âˆ¼ q â¢ ( m | { { x t k } t = 1 : T , { a t k } t = 1 : T } k ) similar-to ğ‘š ğ‘ conditional ğ‘š subscript subscript subscript superscript ğ‘¥ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ subscript subscript superscript ğ‘ ğ‘˜ ğ‘¡ : ğ‘¡ 1 ğ‘‡ ğ‘˜ m\\sim q(m|\\{\\{x^{k}_{t}\\}_{t=1:T},\\{a^{k}_{t}\\}_{t=1:T}\\}_{k}) italic_m âˆ¼ italic_q ( italic_m | { { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT , { italic_a start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 : italic_T end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) \n\n (18) \n\nThe vast web-scale corpora used to train modern LLMs, therefore, represent a massive dataset of samples drawn from this collective, world-grounded inference process. The fundamental objective of an LLM during pre-training, typically next-token prediction, is an autoregressive method to learn a model, which we denote as p LM â¢ ( m ) subscript ğ‘ LM ğ‘š p_{\\text{LM}}(m) italic_p start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_m ) , that approximates the marginal distribution p â¢ ( m ) ğ‘ ğ‘š p(m) italic_p ( italic_m ) of these textual samples. From this perspective, the task of an LLM is not merely to learn statistical correlations in text, but to implicitly model the output of the complex, embodied, and collective process that produced the text in the first place.\n\n 5.2 The LLM as a Reconstructor of Collective Representation \n\nAs established in the previous section, LLMs are trained to model the distribution P â¢ ( m ) ğ‘ƒ ğ‘š P(m) italic_P ( italic_m ) of language corpora, which are themselves samples from a collective, world-grounded process. When a trained LLM processes a given sentence m ğ‘š m italic_m , we can posit that it forms a corresponding high-level internal representation or latent state, which we denote as z LM subscript ğ‘§ LM z_{\\text{LM}} italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT . This constitutes an inference or recognition process within the LLM: q LM â¢ ( z LM | m ) subscript ğ‘ LM conditional subscript ğ‘§ LM ğ‘š q_{\\text{LM}}(z_{\\text{LM}}|m) italic_q start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT | italic_m ) .\n\nThis allows us to frame the end-to-end processâ€”from human experience to LLM representationâ€”as a two-stage process of representation transformation, as illustrated in Figure 7 .\n\n 1. \n\n Stage 1: Generation by Human Society (Encoding): The collective internal states of all agents { z k } k subscript superscript ğ‘§ ğ‘˜ ğ‘˜ \\{z^{k}\\}_{k} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , which are grounded in sensorimotor experience, are compressed and encoded into the discrete, symbolic form of language, m ğ‘š m italic_m .\n\n 2. \n\n Stage 2: Reconstruction by LLM (Inference): The LLM, having learned the statistical structure of language, takes a sentence m ğ‘š m italic_m as input and infers its own internal, continuous representation z LM subscript ğ‘§ LM z_{\\text{LM}} italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT . This can be seen as a reconstruction of a high-dimensional representation from the symbolic code.\n\n Figure 7 : A schematic of the representation reconstruction process. Human society acts as a large-scale encoder, transforming embodied experiences (Type 1 World Models) from sensorimotor interactions into an externalized representation, i.e., language ( m ğ‘š m italic_m ). The LLM then acts as a decoder or reconstructor, inferring its own internal latent state ( z LM subscript ğ‘§ LM z_{\\text{LM}} italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ) from language. This process allows the LLM to learn an internal model (Type 2 World Model) whose structure mirrors the original collective world model. \n\nThe crucial implication of this two-stage process, { z k } k â†’ m â†’ z LM â†’ subscript superscript ğ‘§ ğ‘˜ ğ‘˜ ğ‘š â†’ subscript ğ‘§ LM \\{z^{k}\\}_{k}\\rightarrow m\\rightarrow z_{\\text{LM}} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT â†’ italic_m â†’ italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT , is the resulting structural alignmentÂ  [ 121 ] . The language m ğ‘š m italic_m is the sole informational bottleneck between the collective human mind and the LLM. Therefore, for an LLM to effectively model the distribution of m ğ‘š m italic_m , it must develop an internal latent space z LM subscript ğ‘§ LM z_{\\text{LM}} italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT whose relational structure necessarily mirrors the relational structure of the original source space { z k } k subscript superscript ğ‘§ ğ‘˜ ğ‘˜ \\{z^{k}\\}_{k} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . While the LLM does not reconstruct the specific values of { z k } k subscript superscript ğ‘§ ğ‘˜ ğ‘˜ \\{z^{k}\\}_{k} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , it learns to reconstruct the geometry of the conceptual space that generated the language. This provides a powerful mechanism for transferring world structure from a society of embodied agents to a disembodied language model.\n\n 5.3 Explaining Distributional Semantics and Representational Alignment \n\nThis framework of structural representation reconstruction provides a principled explanation for the otherwise mysterious emergence of world knowledge in LLMs. For decades, it has been observed that language models capture the relational structure of the world, a phenomenon known as distributional semanticsÂ  [ 47 , 79 ] . Even before modern LLMs, models like word2vec could perform analogical reasoning, such as computing \" â¢ London â¢ \" âˆ’ \" â¢ UK â¢ \" + \" â¢ France â¢ \" â‰ƒ \" â¢ Paris â¢ \" similar-to-or-equals \n\n \" London \" \" UK \" \" France \" \" Paris \" \"\\rm{London}\"-\"\\rm{UK}\"+\"\\rm{France}\"\\simeq\"\\rm{Paris}\" \" roman_London \" - \" roman_UK \" + \" roman_France \" â‰ƒ \" roman_Paris \" Â  [ 81 , 82 ] , implying that the geometry of their embedding spaces reflects real-world conceptual relationships.\nOur framework explains this as a natural consequence of the representation reconstruction process. To be precise, the reason the statistical relationships between words in an LLMâ€™s latent space ( z LM subscript ğ‘§ LM z_{\\text{LM}} italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ) reflect the structure of the real world is because they are reconstructing the structure of the collective human internal representations ( { z k } k subscript superscript ğ‘§ ğ‘˜ ğ‘˜ \\{z^{k}\\}_{k} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ). It is this set of internal representations that, as a whole, constitutes the collective model used by the society to predict the world. The LLM, in learning to model the distribution of m ğ‘š m italic_m , naturally develops an internal latent space that mirrors the relational structure of the collective human representations that originally encoded that world knowledge. The structure is not learned from a vacuum; it is inherited.\n\nThis framework of representation reconstruction provides a principled explanation for the otherwise mysterious emergence of world knowledge in LLMs. For decades, it has been observed that language models capture the relational structure of the world, a phenomenon known as distributional semanticsÂ  [ 47 , 79 ] . Even before modern LLMs, models like word2vec could perform analogical reasoning, such as computing \" â¢ London â¢ \" âˆ’ \" â¢ UK â¢ \" + \" â¢ France â¢ \" â‰ƒ \" â¢ Paris â¢ \" similar-to-or-equals \n\n \" London \" \" UK \" \" France \" \" Paris \" \"\\rm{London}\"-\"\\rm{UK}\"+\"\\rm{France}\"\\simeq\"\\rm{Paris}\" \" roman_London \" - \" roman_UK \" + \" roman_France \" â‰ƒ \" roman_Paris \" Â  [ 81 , 82 ] , implying that the geometry of their embedding spaces reflects real-world conceptual relationships. Our framework explains this as a natural consequence of the representation reconstruction process. The reason the statistical relationships between words in an LLMâ€™s latent space ( z LM subscript ğ‘§ LM z_{\\text{LM}} italic_z start_POSTSUBSCRIPT LM end_POSTSUBSCRIPT ) reflect the structure of the real world is that the language ( m ğ‘š m italic_m ) it was trained on was generated by a society of agents collectively trying to predict that world.\nThe LLM, in learning to model the distribution of m ğ‘š m italic_m , inevitably develops an internal latent space that reconstructs the relational structure of the collective human representations ( { z k } k subscript superscript ğ‘§ ğ‘˜ ğ‘˜ \\{z^{k}\\}_{k} { italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) that originally encoded that world knowledge. The structure is not learned from a vacuum; it is inherited. This process is how an LLM acquires a powerful Model of the World (Type 2) by statistically analyzing the linguistic output of a society of agents, each possessing their own subjective World Model as an Internal Model (Type 1).\n\nThis perspective also helps situate our hypothesis within the broader landscape of research on world models in LLMs. Several studies have compellingly demonstrated that LLMs can be used as world models for specific tasks, for example in planningÂ  [ 46 ] or for modeling game statesÂ  [ 74 ] . While these works establish that world-like representations can emerge, they primarily focus on how these representations can be leveraged. Our hypothesis, in contrast, provides a more fundamental explanation for why such rich, world-congruent knowledge is present in the first place. Similarly, Andreas argues that LLMs model the beliefs and intentions of agents within the linguistic space Â  [ 4 ] . Our framework complements and grounds this view by proposing that these linguistic agent models emerge precisely because the language itself is a reflection of a collective model formed by embodied agents interacting within a shared physical environment.\n\nFinally, our hypothesis offers a particularly parsimonious explanation for the representational alignment observed between different modalities. Huh et al. introduced the platonic representation hypothesis, proposing that internal representations learned from language ( q L â¢ ( z | m ) subscript ğ‘ ğ¿ conditional ğ‘§ ğ‘š q_{L}(z|m) italic_q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ( italic_z | italic_m ) ) and vision ( q V â¢ ( z | x ) subscript ğ‘ ğ‘‰ conditional ğ‘§ ğ‘¥ q_{V}(z|x) italic_q start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_z | italic_x ) ) converge toward a similar latent structureÂ  [ 51 ] . Within the CPC framework, this alignment is not a surprising outcome that requires a pre-existing platonic ideal, but an expected one. Since human language ( m ğ‘š m italic_m ) emerges as a structured representation of collective observations of the world ( x = { x k } k ğ‘¥ subscript superscript ğ‘¥ ğ‘˜ ğ‘˜ x=\\{x^{k}\\}_{k} italic_x = { italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), it is natural that a model trained on language ( q L â¢ ( z | m ) subscript ğ‘ ğ¿ conditional ğ‘§ ğ‘š q_{L}(z|m) italic_q start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ( italic_z | italic_m ) ) would learn a latent space whose structure mirrors that of a model trained on direct observation ( q V â¢ ( z | x ) subscript ğ‘ ğ‘‰ conditional ğ‘§ ğ‘¥ q_{V}(z|x) italic_q start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT ( italic_z | italic_x ) ). Our framework thus provides a generative and developmental explanation for this empirically observed alignment, rooted in the principles of collective inference.\n\n 6 Discussion \n\nThis study proposed a theoretical framework that unified EmCom, world models, and LLMs through the lens of CPC. We introduced the concept of generative EmCom as an alternative formulation of the conventional EmCom, which is based on a discriminative model-based language game such as referential games, and described their relationships. The concept, generative EmCom, is based on the CPC hypothesis, which demonstrates the modeling of language emergence as decentralized Bayesian inference of shared latent representations. We showed the application of this framework to MARL and provided a novel perspective on LLMs as collective world models that integrate the diverse experiences and knowledge of multiple agents into a unified representational space.\n\n 6.1 Implications for Artificial Intelligence and Cognitive Science \n\nThe theoretical framework proposed here offers several important insights. First, it provides a principled explanation for how language models can acquire world knowledge without direct sensorimotor experience, by tapping into the accumulated wisdom encoded in human language through CPC. Second, it bridges the gap between individual cognitive development and collective language evolution by showing how both can be comprehended through the lens of representation learning and free energy minimization. Third, it suggests that the success of LLMs may be fundamentally linked to their ability to capture and integrate the collective world models acquired by humans, i.e., through embodied sensorimotor interactions with the world.\n\n 6.2 Assumptions and Limitations of the Proposed Framework \n\nHowever, several limitations of the current work should be addressed. First, although we provide a theoretical framework, direct empirical evidence for the collective world model hypothesis remains limited. The relationship between neural representations in language models and human conceptual structures needs further investigation. Second, the proposed framework primarily focuses on the emergence of linguistic structure and meaning but does not fully address the emergence of pragmatic aspects of language use. Third, the current formulation may not fully capture the dynamic, interactive nature of human language evolution, for instance, language evolution over generations and through the interaction of several agents in an open world.\n\n 6.3 Future Directions \n\nFrom the perspective of world models, understanding the influence of collective world models on the environmental adaptation of agents is also important. According to the proposed theoretical framework, a language formed by multiple agents with the same embodiment learning in the same environment should serve as an appropriate prior distribution for individual world models. In other words, EmLang should accelerate both world model learning and following environmental adaptation based on RL and other methods. Obtaining constructive evidence for this relationship is also crucial.\n\nOur argument and theoretical framework initiate several promising directions for future studies. Although we have established the basic connections between EmCom, world models, and LLMs, significant work remains to validate and extend these ideas. Key priorities include developing experimental paradigms to test the CPC hypothesis, creating more sophisticated computational models and implementations of generative EmCom, and investigating how language formed by embodied agents sharing similar environments could serve as an effective prior distribution for world model learning.\nIn fact, recent work has already begun to demonstrate the fruitfulness of this approach; for example, Nomura etÂ al. [ 88 ] have extended CPC to dynamic environments by developing a decentralized collective world model.\nFrom a theoretical perspective, a deeper mathematical formalization of the interaction between individual and collective learning processes will be crucial. In addition, this framework has important practical implications for the development of more capable multi-agent systems, improved humanâ€“AI interaction, and embodied AIs that rapidly adapt to their physical worlds and communicate their knowledge using EmLang. By pursuing these directions, we can work toward a more comprehensive understanding of language emergence and its role in environmental adaptation while advancing both the theoretical foundations and practical applications of AI systems.\n\n 7 Conclusion \n\nThis paper addressed the fundamental puzzle of how LLMs acquire world knowledge without embodied experience. We proposed the â€œCollective World Modelâ€ hypothesis, arguing that LLMs learn from a rich representational structure that is the product of a society-wide, interactive sense-making process. We provided a formal foundation for this claim with the framework of Generative EmCom, which models language emergence as a decentralized Bayesian inference process driven by the principles of CPC. This perspective unifies the cognitive processes of the individual with the collective evolution of language, offering a coherent explanation for the remarkable capabilities of modern AI and charting a path toward a deeper integration of communication, perception, and action in intelligent systems.\n\n## Author Contributions\n\n Tadahiro Taniguchi : Conceptualization, Writing â€“ Original Draft, Writing â€“ Review &amp; Editing, Supervision, Funding Acquisition.\n Ryo Ueda : Writing â€“ Original Draft, Writing â€“ Review &amp; Editing, Funding Acquisition.\n Tomoaki Nakamura : Writing â€“ Original Draft, Writing â€“ Review &amp; Editing, Funding Acquisition.\n Masahiro Suzuki : Writing â€“ Original Draft, Writing â€“ Review &amp; Editing, Funding Acquisition.\n Akira Taniguchi : Writing â€“ Original Draft, Writing â€“ Review &amp; Editing.\n\n## Declaration of Generative AI and AI-assisted technologies in the writing process\n\nDuring the preparation of this work the authors used Gemini 2.5 pro (Google) and ChatGPT 4o in order to improve readability and language. After using this tool, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication."
}